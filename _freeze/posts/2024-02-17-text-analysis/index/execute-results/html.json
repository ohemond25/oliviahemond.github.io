{
  "hash": "11fadfbd3a2f35a64240391e861538f5",
  "result": {
    "markdown": "---\ntitle: \"Text Sentiment Analysis\"\ndescription: \"Extracting text and performing a sentiment analysis on Harry Potter and the Prisoner of Azkaban\"\nauthor: \"Olivia Hemond\"\ndate: 02-17-2024\nimage: fig4.png\ndraft: false\ncategories: [R, Text, Visualization]\nformat: \n  html:\n    code-fold: true\n    toc: true\n    number-sections: false\n    embed-resources: true\neditor: visual\ntheme: lux\nexecute:\n  echo: true\n  message: false\n  warning: false\n---\n\n\n# Harry Potter and the Prisoner of Azkaban\n\n![](hp3cover.png){fig-align=\"center\" width=\"455\"}\n\n## Text Analysis Overview\n\n### Data Summary\n\nThis analysis uses the text from the book \"Harry Potter and the Prisoner of Azkaban\" by J.K. Rowling. The story follows Harry through his third year at Hogwarts, as he learns to fight Dementors, sneak around using the Marauder's Map, and even travel in time! By the end of the story, Harry learns a lot about the mysterious escaped criminal Sirius Black and about his own parents' past.\n\nSource: **Rowling, J. K.** Harry Potter and the Prisoner of Azkaban. New York: Arthur A. Levine Books, 1999. [Full text here](http://vidyaprabodhinicollege.edu.in/VPCCECM/ebooks/ENGLISH%20LITERATURE/Harry%20potter/(Book%203)%20Harry%20Potter%20And%20The%20Prisoner%20Of%20Azkaban_001.pdf).\n\n### Purpose\n\nThis analysis had two main goals:\n\n1.  Find and visualize the most common words within each chapter and in the book as a whole\n\n2.  Perform a sentiment analysis to visualize the tone (positive or negative) of each chapter of the book\n\n### Analytical Outline\n\n1.  Import & Tidy Text\n\n    -   Prepare the data\n\n        -   Read in the complete PDF of Harry Potter and the Prisoner of Azkaban\n\n        -   Add column for page number\n\n        -   Create character strings for each line of text in the whole book\n\n        -   Let each line of text be its own row in the dataframe\n\n        -   Remove extra whitespaces\n\n        -   Add column for chapter number (in numeric format)\n\n    -   Extract every individual word\n\n        -   Let each word in the text be its own row in the dataframe (associated with the page and chapter from which it came)\n\n    -   Remove stop words\n\n        -   Stop words are commonly used words that don't carry significant meaning (e.g., \"of\", \"a\", \"the\", \"and\")\n\n        -   Remove stop words from this dataset\n\n2.  Find Most-Used Words\n\n    -   Calculate the five most-used words in each chapter and visualize them\n\n    -   Graph the frequency of different character names being mentioned throughout the book\n\n    -   Find the top 100 most-used words in the entire book and visualize them as a word cloud\n\n3.  Perform Sentiment Analysis\n\n    -   Use the \"afinn\" lexicon to assign each word a value on the positive/negative scale\n\n        -   -5 being most negative, 5 being most positive\n\n    -   For each chapter, take a weighted average of the positivity/negativity scores (weighted by the amount of times each word was used)\n\n    -   Visualize how the tone of the book changes in each chapter\n\n## Import & Tidy Text\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(here)\nlibrary(tidytext)\nlibrary(pdftools)\nlibrary(ggwordcloud)\nlibrary(textdata)\n```\n:::\n\n\n#### Prepare the text data\n\n\n::: {.cell}\n\n```{.r .cell-code}\n### Read in\nhp3_text <- pdftools::pdf_text(here('posts', '2024-02-17-text-analysis', 'data', 'hp_3.pdf'))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n### Wrangle and tidy\nhp3_lines <- data.frame(hp3_text) %>% \n  mutate(page = 1:n() - 1) %>% \n  mutate(text_full = str_split(hp3_text, pattern = '\\\\n')) %>%  # creates character strings of each line\n  unnest(text_full) %>%  # make row for each line of text\n  select(!hp3_text) %>% # don't need original data column anymore\n  mutate(text_full = str_squish(text_full)) # remove any extra whitespace\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n### Add chapters in separate column\nhp3_chapters <- hp3_lines %>% \n  slice(-1) %>% # remove empty first row\n  mutate(chapter = ifelse(str_detect(text_full, \"CHAPTER\"), text_full, NA)) %>% # creates new chapter column\n  mutate(chapter = str_remove(chapter, \"CHAPTER\")) %>% # remove the word \"chapter\"\n  fill(chapter, .direction = 'down') %>% # assign chapter to all entries in each chapter\n  mutate(chapter_num = case_when(\n    chapter == \" ONE\" ~ 1,\n    chapter == \" TWO\" ~ 2,\n    chapter == \" THREE\" ~ 3,\n    chapter == \" FOUR\" ~ 4,\n    chapter == \" FIVE\" ~ 5,\n    chapter == \" SIX\" ~ 6,\n    chapter == \" SEVEN\" ~ 7,\n    chapter == \" EIGHT\" ~ 8,\n    chapter == \" NINE\" ~ 9,\n    chapter == \" TEN\" ~ 10,\n    chapter == \" ELEVEN\" ~ 11,\n    chapter == \" TWELVE\" ~ 12,\n    chapter == \" THIRTEEN\" ~ 13,\n    chapter == \" FOURTEEN\" ~ 14,\n    chapter == \" FIFTEEN\" ~ 15,\n    chapter == \" SIXTEEN\" ~ 16,\n    chapter == \" SEVENTEEN\" ~ 17,\n    chapter == \" EIGHTEEN\" ~ 18,\n    chapter == \" NINETEEN\" ~ 19,\n    chapter == \" TWENTY\" ~ 20,\n    chapter == \" TWENTY-ONE\" ~ 21,\n    chapter == \" TWENTY-TWO\" ~ 22\n    )) # change written chapter numbers into actual numbers\n```\n:::\n\n\n#### Get words and wordcount\n\n\n::: {.cell}\n\n```{.r .cell-code}\n### Get words\nhp3_words <- hp3_chapters %>% \n  unnest_tokens(word, text_full) %>% \n  select(page, chapter_num, word) %>% \n  mutate(word = str_split_i(word, pattern = \"'s\", 1)) # some have 's, (harry's), want this to count with root word\n\n### Wordcount\nhp3_wordcount <- hp3_words %>% \n  count(chapter_num, word)\n```\n:::\n\n\n#### Remove stop words\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhp3_wordcount_clean <- hp3_wordcount %>% \n  anti_join(stop_words, by = \"word\") \n```\n:::\n\n\n## Most-Used Words\n\n#### By Chapter\n\n\n::: {.cell}\n\n```{.r .cell-code}\n### Top 5 words for each chapter\ntop_5_words <- hp3_wordcount_clean %>% \n  group_by(chapter_num) %>% \n  arrange(-n) %>% \n  slice(1:5) %>% \n  ungroup()\n\n### Plot\nggplot(top_5_words, aes(x = n, y = word)) +\n  geom_col(fill = \"#740001\") +\n  facet_wrap(~as.factor(chapter_num), scales = \"free\") +\n  labs(x = \"\", y = \"\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![The top 5 words used in each chapter in the book. Bar sizes depict the amount of times each word was used.](index_files/figure-html/fig-topwordschapters-1.png){#fig-topwordschapters width=768}\n:::\n:::\n\n\nMany of the most used words are the names of characters, which makes sense given it's a book with a lot of dialogue and third-person narration. Using character names as a proxy for their relevance in any given chapter, we can track how certain characters appear / disappear from the narrative:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n### Look at some key characters over the course of the book\nhp3_character_count <- hp3_wordcount_clean %>% \n  filter(word %in% c(\"hagrid\", \"lupin\", \"buckbeak\", \"snape\", \"pettigrew\", \"sirius\"))\n\nhp3_words_by_chap <- hp3_wordcount_clean %>% \n  group_by(chapter_num) %>% \n  summarize(word_count = sum(n))\n\nhp3_characters_by_chap <- left_join(hp3_character_count, hp3_words_by_chap, by = \"chapter_num\") %>% \n  mutate(freq_per_chap = n/word_count)\n\n### Plot\nggplot(hp3_characters_by_chap, aes(x = as.factor(chapter_num), y = freq_per_chap, color = word, group = word)) +\n  geom_point() +\n  geom_line() +\n  labs(x = \"\", y = \"Frequency\") +\n  facet_wrap(~word, scales = \"free_y\", nrow = 3) +\n  scale_color_manual(values = c(\"#740001\", \"#AE0001\", \"#EEBA30\", \"#D3A625\", \"#000000\", \"darkgreen\")) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![The frequency of Buckbeak, Hagrid, Lupin, Pettigrew, Sirius, and Snape being mentioned in each chapter of the book. The frequency was calculated by dividing the number of uses of each word by the total number of words in that chapter.](index_files/figure-html/fig-characters-1.png){#fig-characters width=1056}\n:::\n:::\n\n\n#### Entire Book\n\n\n::: {.cell}\n\n```{.r .cell-code}\n### Top 100 words from whole book\nhp3_top100 <- hp3_wordcount_clean %>% \n  group_by(word) %>% \n  summarize(n = sum(n)) %>% \n  arrange(-n) %>% \n  slice(1:100)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n### Create wordcloud\nggplot(data = hp3_top100, aes(label = word)) +\n  geom_text_wordcloud(aes(color = n, size = n+500), shape = \"star\") +\n  scale_size_area(max_size = 6) +\n  scale_color_gradientn(colors = c(\"#740001\",\"#D3A625\",\"#AE0001\")) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![Wordcloud of the top 100 words used in the book, sized by their number of uses.](index_files/figure-html/fig-wordcloud-1.png){#fig-wordcloud width=672}\n:::\n:::\n\n\n## Sentiment Analysis\n\n\n::: {.cell}\n\n```{.r .cell-code}\n### Read in the 'afinn' lexicon\nafinn_lex <- get_sentiments(lexicon = \"afinn\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n### Join the lexicon to our words\nhp3_afinn <- hp3_words %>% \n  inner_join(afinn_lex, by = 'word')\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n### Count the number of words in each chapter assigned to each value (from -5 to 5)\nafinn_counts <- hp3_afinn %>% \n  group_by(chapter_num, value) %>%\n  summarize(n = n())\n\n### Take a weighted average of the values (using the number of words to weight)\nafinn_mean <- afinn_counts %>% \n  summarize(weighted_avg_value = weighted.mean(value, n))\n\n### Plot \nggplot(data = afinn_mean) +\n  geom_col(aes(x = as.factor(chapter_num), y = weighted_avg_value, fill = weighted_avg_value > 0)) +\n  labs(x = \"Chapter\", y = \"Average Word Positivity\") +\n  scale_fill_manual(name = 'Positive?', values = setNames(c('#D3A625','#AE0001'), c(T, F))) +\n  theme_minimal() +\n  theme(legend.position = \"none\") \n```\n\n::: {.cell-output-display}\n![Sentiment analysis of the average positivity of each chapter in the book. Values above 0 indicate the chapter had an overall positive tone. Values below 0 indicate a negative tone. Average positivity was calculated by weighting the positivity score of words by their amount of useage.](index_files/figure-html/fig-sentiments-1.png){#fig-sentiments width=672}\n:::\n:::\n\n\nThe first portion of the book has some overall positive chapters (like Chapter 4, where Harry returns to school and reunites with his friends) and some more negative chapters (like Chapters 2 and 3, where Harry accidentally inflates his Aunt Marge like a balloon, and then must run away and catch the chaotic Knight Bus). The latter half of the book takes on a much heavier and more negative tone, with the most negative chapter being Chapter 17, where Harry, Ron, and Hermione find themselves caught in the Shrieking Shack amidst a showdown between Sirius Black, Professor Lupin, Professor Snape, and Peter Pettigrew (as the rat Scabbers). The book ultimately ends on a positive note in the final chapter, once Sirius and Buckbeak have safely escaped from their respective death sentences!\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}