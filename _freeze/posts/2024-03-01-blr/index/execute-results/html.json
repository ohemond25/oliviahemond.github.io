{
  "hash": "b74e84d3a3bd80a16a952d5f46fd18ab",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Machine Learning Algorithms Can Accurately Classify Plant Species\"\ndescription: \"Using a binary logistic regression model to classify plant species\"\nauthor: \"Olivia Hemond\"\ndate: 03-01-2024\nimage: serenoa_repens.jpg\ndraft: false\ncategories: [Data Science]\nformat: \n  html:\n    code-fold: true\n    toc: true\n    embed-resources: true\neditor: visual\nexecute: \n  echo: true\n  message: false\n  warning: false\n---\n\n\n\n# Overview\n\nThis dataset describes the survival and growth of two species of palmetto (*Serenoa repens* and *Sabal etonia*), as measured at Archbold Biological Station in Florida. Both species are types of fan palm. The scientists studying these species measured plant height, canopy length and width, number of green leaves, and other characteristics of the plants' growth. Data were collected annually from 1981 - 1997, and then again in 2001 and in 2017.\n\nThe purpose of this analysis is to test whether measurements of height, canopy length, canopy width, and number of green leaves can be used to classify whether an unknown plant is a member of *S. repens* or *S. etonia*. I used binary logistic regression to create the classification model.\n\nMy analytical process went as follows:\n\n1.  Obtain and tidy data\n\n    -   Read in and clean the data\n\n2.  Explore and visualize data\n\n    -   Create boxplots to compare the distribution of values for each variable between the two species\n\n3.  Compare two binary logistic regression models\n\n    -   Define the two model formulas\n\n    -   Use ten-fold cross validation to fit the models\n\n    -   Compare the predictive performance of the two models\n\n4.  Train selected model\n\n    -   Use entire, clean dataset to train the model and obtain finalized parameter results\n\n5.  Present classification results\n\n    -   Generate species predictions based on best model\n\n    -   Calculate the number of correct and incorrect predictions for each species\n\n    -   Calculate the percent of correct predictions for each species\n\nSee the corresponding sections in this analysis for more detailed descriptions of the steps involved in this analysis.\n\n**Data source:** Abrahamson, W.G. 2019. Survival, growth and biomass estimates of two dominant palmetto species of south-central Florida from 1981 - 2017, ongoing at 5-year intervals ver 1. Environmental Data Initiative. <https://doi.org/10.6073/pasta/f2f96ec76fbbd4b9db431c79a770c4d5>\n\n# Obtain and Tidy Data\n\n### Libraries\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(here)\nlibrary(cowplot)\nlibrary(knitr)\nlibrary(kableExtra)\n```\n:::\n\n\n\n### Read in data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np_df <- read_csv(here(\"posts\", \"2024-03-01-blr\", \"data\", \"palmetto.csv\"))\n```\n:::\n\n\n\n### Clean data\n\nI removed unnecessary columns and assigned factor levels to my two species ID numbers. Species 1 = *S. repens* and Species 2 = *S. etonia*\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np_clean <- p_df %>% \n  select(species, height:green_lvs) %>% \n  mutate(species = as_factor(species)) %>% \n  drop_na()\n```\n:::\n\n\n\n# Exploratory Visualizations\n\nI am interested in whether height, width, length, and number of green leaves are good variables to use to differentiate between my two species. To be good predictors, there should be notable differences in these values between the species. I decided to explore these four variables using boxplots.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nheight_plot <- ggplot(data = p_clean) + \n  geom_boxplot(aes(x = as_factor(species), y = height), fill = \"olivedrab3\", color = \"olivedrab\") +\n  scale_x_discrete(labels = c(\"1\" = \"S. repens\", \"2\" = \"S. etonia\")) +\n  labs(x = \"\", y = \"Height (cm)\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(face = \"italic\"))\n\nwidth_plot <- ggplot(data = p_clean) + \n  geom_boxplot(aes(x = as_factor(species), y = width), fill = \"olivedrab3\", color = \"olivedrab\") +\n  scale_x_discrete(labels = c(\"1\" = \"S. repens\", \"2\" = \"S. etonia\")) +\n  labs(x = \"\", y = \"Width (cm)\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(face = \"italic\"))\n\nlength_plot <- ggplot(data = p_clean) + \n  geom_boxplot(aes(x = as_factor(species), y = length), fill = \"olivedrab3\", color = \"olivedrab\") +\n  scale_x_discrete(labels = c(\"1\" = \"S. repens\", \"2\" = \"S. etonia\")) +\n  labs(x = \"\", y = \"Length (cm)\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(face = \"italic\"))\n\nleaves_plot <- ggplot(data = p_clean) + \n  geom_boxplot(aes(x = as_factor(species), y = green_lvs), fill = \"olivedrab3\", color = \"olivedrab\") +\n  scale_x_discrete(labels = c(\"1\" = \"S. repens\", \"2\" = \"S. etonia\")) +\n  labs(x = \"\", y = \"Number of Green Leaves\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(face = \"italic\"))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_grid(height_plot, width_plot, length_plot, leaves_plot,\n          labels = c(\"A\", \"B\", \"C\", \"D\"),\n          ncol = 2,\n          vjust = 1)\n```\n\n::: {.cell-output-display}\n![Boxplots comparing height, width, length, and number of green leaves for *S. repens* and *S. etonia.*](index_files/figure-html/fig-variables-1.png){#fig-variables width=672}\n:::\n:::\n\n\n\n**Takeaways:**\n\nPlant heights between the two species look fairly similar, so height may not be a good predictor of species. There may be some difference in plant widths, and even more noticeable differences in length and number of green leaves. I expect length and green leaves to be especially important predictors, since they seem to show the largest differences between species.\n\n# Binary Logistic Regression Models\n\n### Create formulas\n\nI am interested in comparing two models. One model predicts species based on height, length, width, and green leaves (Model 1). The other model predicts species based only on height, width, and green leaves (Model 2). The difference between the two is whether length is used as a predictor variable.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# model 1\nf1 <- species ~ height + length + width + green_lvs\n# model 2\nf2 <- species ~ height + width + green_lvs\n```\n:::\n\n\n\n### Ten-Fold Cross Validation\n\nNow that the two model formulas are defined, it is time to see which performs better at classification. I decided to use ten-fold cross validation to repeatedly fit the models to ten different subsets of the data, and then I extracted the average performance metrics from each model.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create folded version of dataset\nset.seed(10101)\np_folds <- vfold_cv(p_clean, v = 10, repeats = 10)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# set up model\nblr_mdl <- logistic_reg() %>% \n  set_engine('glm')\n\n# create workflows\nblr_wf_1 <- workflow() %>% \n  add_model(blr_mdl) %>% \n  add_formula(f1) \n\nblr_wf_2 <- workflow() %>% \n  add_model(blr_mdl) %>% \n  add_formula(f2)\n\n# apply the workflows to the folded data\nblr_fit_folds_1 <- blr_wf_1 %>% \n  fit_resamples(p_folds)\nblr_fit_folds_2 <- blr_wf_2 %>% \n  fit_resamples(p_folds)\n```\n:::\n\n\n\n### Cross Validation Results\n\n\n\n::: {#tbl-metrics .cell layout-nrow=\"2\" tbl-cap='Comparing the predictive performance of the two models. The results for the accuracy and area under the ROC curve are presented for both Model 1 (a) and Model 2 (b).' tbl-subcap='[\"Model 1\",\"Model 2\"]'}\n\n```{.r .cell-code}\ncollect_metrics(blr_fit_folds_1) %>% \n  select(-.config) %>% \n  rename(metric = .metric,\n         estimator = .estimator,\n         standard_error = std_err) %>% \n  kbl() %>% \n  kable_styling(\"basic\", position = \"center\")\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> metric </th>\n   <th style=\"text-align:left;\"> estimator </th>\n   <th style=\"text-align:right;\"> mean </th>\n   <th style=\"text-align:right;\"> n </th>\n   <th style=\"text-align:right;\"> standard_error </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> accuracy </td>\n   <td style=\"text-align:left;\"> binary </td>\n   <td style=\"text-align:right;\"> 0.9168905 </td>\n   <td style=\"text-align:right;\"> 100 </td>\n   <td style=\"text-align:right;\"> 0.0007788 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> roc_auc </td>\n   <td style=\"text-align:left;\"> binary </td>\n   <td style=\"text-align:right;\"> 0.9725080 </td>\n   <td style=\"text-align:right;\"> 100 </td>\n   <td style=\"text-align:right;\"> 0.0003592 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n\n```{.r .cell-code}\ncollect_metrics(blr_fit_folds_2) %>% \n  select(-.config) %>% \n  rename(metric = .metric,\n         estimator = .estimator,\n         standard_error = std_err) %>% \n  kbl() %>% \n  kable_styling(\"basic\", position = \"center\")\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> metric </th>\n   <th style=\"text-align:left;\"> estimator </th>\n   <th style=\"text-align:right;\"> mean </th>\n   <th style=\"text-align:right;\"> n </th>\n   <th style=\"text-align:right;\"> standard_error </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> accuracy </td>\n   <td style=\"text-align:left;\"> binary </td>\n   <td style=\"text-align:right;\"> 0.8988665 </td>\n   <td style=\"text-align:right;\"> 100 </td>\n   <td style=\"text-align:right;\"> 0.0008640 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> roc_auc </td>\n   <td style=\"text-align:left;\"> binary </td>\n   <td style=\"text-align:right;\"> 0.9634580 </td>\n   <td style=\"text-align:right;\"> 100 </td>\n   <td style=\"text-align:right;\"> 0.0004167 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n**Results:**\n\nBased on the results of the cross validation, I am choosing Model 1 as the better model. Model 1 has a greater area under the receiver operating characteristic (ROC) curve than Model 2 has (0.9725 compared with 0.9635). The model with a greater area under the curve is the better classifier. In addition, Model 1 had a slightly higher accuracy rate than Model 2 (91.7% compared with 89.9%). Though Model 1 ended up winning based on area under the curve and accuracy, it is worth noting that both models performed very well.\n\n# Train Selected Model\n\nI next trained Model 1 using the entire dataset (without any folding) to obtain my final coefficient results for each predictor variable (shown in the \"estimate\" column of the table below).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## run model on entire dataset\nblr1_fit <- blr_mdl %>% \n  fit(formula = f1, data = p_clean)\n```\n:::\n\n::: {#tbl-results .cell tbl-cap='Final Model 1 results. Coefficient values for the intercept, as well as for each of the four predictor variables, are shown. Standard errors and p-values for the coefficients are also included.'}\n\n```{.r .cell-code}\n# create table with blr results\nbroom::tidy(blr1_fit) %>% \n  select(-statistic) %>% \n  kbl() %>% \n  kable_styling(\"basic\", position = \"center\")\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> term </th>\n   <th style=\"text-align:right;\"> estimate </th>\n   <th style=\"text-align:right;\"> std.error </th>\n   <th style=\"text-align:right;\"> p.value </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> (Intercept) </td>\n   <td style=\"text-align:right;\"> 3.2266851 </td>\n   <td style=\"text-align:right;\"> 0.1420708 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> height </td>\n   <td style=\"text-align:right;\"> -0.0292173 </td>\n   <td style=\"text-align:right;\"> 0.0023061 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> length </td>\n   <td style=\"text-align:right;\"> 0.0458233 </td>\n   <td style=\"text-align:right;\"> 0.0018661 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> width </td>\n   <td style=\"text-align:right;\"> 0.0394434 </td>\n   <td style=\"text-align:right;\"> 0.0021000 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> green_lvs </td>\n   <td style=\"text-align:right;\"> -1.9084747 </td>\n   <td style=\"text-align:right;\"> 0.0388634 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n# Classification Results\n\nUsing my finalized Model 1, I decided to evaluate its predictive strength. I generated predicted species classifications for each observed set of plant height, width, length, and green leaves in the dataset, then compared these predictions to the actual species identities. Predictions were made using a 50% cutoff, meaning that a species was classified as species 1 if the probability of it being species 1, based on my Model 1 results, was 50% or greater.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# generate predictions\np_predict <- p_clean %>% \n  mutate(predict(blr1_fit, new_data = .)) %>% \n  mutate(predict(blr1_fit, new_data = ., type = 'prob'))\n\n# make table of prediction results\npredict_table <- table(p_predict %>%\n        select(species, .pred_class))\n```\n:::\n\n::: {#tbl-predictions .cell tbl-cap='Model 1 predictions. For each species, the number of correct predictions and incorrect predictions are shown. The leftmost column depicts the percent of predictions that were correct.'}\n\n```{.r .cell-code}\nkbl(data.frame(\n  species = c(\"1\", \"2\"),\n  n_correct = c(5548, 5701),\n  n_incorrect = c(564, 454)) %>% \n  mutate(p_correct = n_correct/(n_correct+n_incorrect))) %>% \n  kable_styling(\"basic\", position = \"center\")\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> species </th>\n   <th style=\"text-align:right;\"> n_correct </th>\n   <th style=\"text-align:right;\"> n_incorrect </th>\n   <th style=\"text-align:right;\"> p_correct </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> 1 </td>\n   <td style=\"text-align:right;\"> 5548 </td>\n   <td style=\"text-align:right;\"> 564 </td>\n   <td style=\"text-align:right;\"> 0.9077225 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 2 </td>\n   <td style=\"text-align:right;\"> 5701 </td>\n   <td style=\"text-align:right;\"> 454 </td>\n   <td style=\"text-align:right;\"> 0.9262388 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n**Conclusion:**\n\nMy fitted Model 1 classified observations of species 1 (*S. repens*) correctly 91% of the time, and of species 2 (*S. etonia*) correctly 93% of the time. While the model is not perfect, it does have a very high accuracy rate. The model could potentially be improved if other strong predictor variables could be found and included.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}