[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Olivia Hemond",
    "section": "",
    "text": "I am a master’s student in Environmental Science and Management at the Bren School at UCSB, specializing in Conservation Planning and Strategic Environmental Communications. I’m focusing my degree on climate adaptation, ecological resilience, and data science. For my master’s capstone project, my team and I will provide a forest collaborative in Colorado with the information and tools needed to implement adaptive forest management practices. I also hold a B.S. in Molecular Environmental Biology from UC Berkeley, where I studied how global environmental change is impacting organisms, ecosystems, and human well-being. I have professional experience working as a research assistant, a teacher, and an environmental analyst.\nI hope to use my cross-disciplinary expertise to bridge the gap between science and action. My goal is to work as an environmental planner, specializing in climate adaptation, community resilience, and hazard mitigation.\nThough I’ve spent a number of years living in California, and one in Spain, I was raised in Massachusetts and that’s where I call home. In my free time, I love exploring the outdoors, trying new restaurants, making art, and traveling.\n\nExperienceEducationConferences\n\n\nDirectorate Resource Fellow (Summer 2024) | U.S. Fish & Wildlife Service\nArnhold Environmental Graduate Fellow (2023 - present) | Environmental Markets Lab (emLab), UCSB\nEnvironmental Analyst (2022 - 2023) | ERG\n\n\nMaster in Environmental Science and Management (Expected 2025) | Bren School of Environmental Science & Management, UCSB\nBS in Molecular Environmental Biology (2021) | UC Berkeley\n\n\nNational Adaptation Forum, May 2024, St. Paul MN (attendee)\nPoint Conception Institute Science Symposium, March 2024, Santa Barbara CA (attendee)\nEmerging Researchers National Conference in STEM, February 2020, Washington D.C. (presenter)\nSEA Fellows Summer Science Symposium, August 2019, Walpole ME (presenter)"
  },
  {
    "objectID": "about.html#about-me",
    "href": "about.html#about-me",
    "title": "Olivia Hemond",
    "section": "",
    "text": "I am a master’s student in Environmental Science and Management at the Bren School at UCSB, specializing in Conservation Planning and Strategic Environmental Communications. I’m focusing my degree on climate adaptation, ecological resilience, and data science. For my master’s capstone project, my team and I will provide a forest collaborative in Colorado with the information and tools needed to implement adaptive forest management practices. I also hold a B.S. in Molecular Environmental Biology from UC Berkeley, where I studied how global environmental change is impacting organisms, ecosystems, and human well-being. I have professional experience working as a research assistant, a teacher, and an environmental analyst.\nI hope to use my cross-disciplinary expertise to bridge the gap between science and action. My goal is to work as an environmental planner, specializing in climate adaptation, community resilience, and hazard mitigation.\nThough I’ve spent a number of years living in California, and one in Spain, I was raised in Massachusetts and that’s where I call home. In my free time, I love exploring the outdoors, trying new restaurants, making art, and traveling.\n\nExperienceEducationConferences\n\n\nDirectorate Resource Fellow (Summer 2024) | U.S. Fish & Wildlife Service\nArnhold Environmental Graduate Fellow (2023 - present) | Environmental Markets Lab (emLab), UCSB\nEnvironmental Analyst (2022 - 2023) | ERG\n\n\nMaster in Environmental Science and Management (Expected 2025) | Bren School of Environmental Science & Management, UCSB\nBS in Molecular Environmental Biology (2021) | UC Berkeley\n\n\nNational Adaptation Forum, May 2024, St. Paul MN (attendee)\nPoint Conception Institute Science Symposium, March 2024, Santa Barbara CA (attendee)\nEmerging Researchers National Conference in STEM, February 2020, Washington D.C. (presenter)\nSEA Fellows Summer Science Symposium, August 2019, Walpole ME (presenter)"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Project Portfolio",
    "section": "",
    "text": "Hierarchical Clustering\n\n\n\nR\n\n\nVisualization\n\n\nClustering\n\n\n\nUsing agglomerative hierarchical clustering to group stream sites by water chemistry\n\n\n\nOlivia Hemond\n\n\nMar 17, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrincipal Components Analysis\n\n\n\nR\n\n\nVisualization\n\n\nPCA\n\n\n\nUsing PCA to determine the relationship between historical redlining practices and present-day environmental health inequities\n\n\n\nOlivia Hemond\n\n\nMar 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNon-Linear Least Squares\n\n\n\nR\n\n\nVisualization\n\n\nModeling\n\n\n\nDescribing and predicting crop yields using fitted non-linear least squares models\n\n\n\nOlivia Hemond\n\n\nMar 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBinary Logistic Regression\n\n\n\nR\n\n\nModeling\n\n\nRegression\n\n\nVisualization\n\n\nCross-Validation\n\n\n\nUsing a binary logistic regression model to classify plant species\n\n\n\nOlivia Hemond\n\n\nMar 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nText Sentiment Analysis\n\n\n\nR\n\n\nText\n\n\nVisualization\n\n\n\nExtracting text and performing a sentiment analysis on Harry Potter and the Prisoner of Azkaban\n\n\n\nOlivia Hemond\n\n\nFeb 17, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpatial Data Analysis\n\n\n\nR\n\n\nSpatial\n\n\nVisualization\n\n\n\nAnalyzing the spatial distribution of inland oil spills in California\n\n\n\nOlivia Hemond\n\n\nFeb 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTime Series Analysis\n\n\n\nR\n\n\nVisualization\n\n\nTime Series\n\n\n\nAssessing temporal patterns of salmon and steelhead trout migration\n\n\n\nOlivia Hemond\n\n\nFeb 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Wrangling and Visualization\n\n\n\nR\n\n\nVisualization\n\n\n\nInvestigating mountain yellow-legged frog abundance\n\n\n\nOlivia Hemond\n\n\nJan 30, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024-03-01-blr/index.html",
    "href": "posts/2024-03-01-blr/index.html",
    "title": "Binary Logistic Regression",
    "section": "",
    "text": "Overview\nThis dataset describes the survival and growth of two species of palmetto (Serenoa repens and Sabal etonia), as measured at Archbold Biological Station in Florida. Both species are types of fan palm. The scientists studying these species measured plant height, canopy length and width, number of green leaves, and other characteristics of the plants’ growth. Data were collected annually from 1981 - 1997, and then again in 2001 and in 2017.\nThe purpose of this analysis is to test whether measurements of height, canopy length, canopy width, and number of green leaves can be used to classify whether an unknown plant is a member of S. repens or S. etonia. I used binary logistic regression to create the classification model.\nMy analytical process went as follows:\n\nObtain and tidy data\n\nRead in and clean the data\n\nExplore and visualize data\n\nCreate boxplots to compare the distribution of values for each variable between the two species\n\nCompare two binary logistic regression models\n\nDefine the two model formulas\nUse ten-fold cross validation to fit the models\nCompare the predictive performance of the two models\n\nTrain selected model\n\nUse entire, clean dataset to train the model and obtain finalized parameter results\n\nPresent classification results\n\nGenerate species predictions based on best model\nCalculate the number of correct and incorrect predictions for each species\nCalculate the percent of correct predictions for each species\n\n\nSee the corresponding sections in this analysis for more detailed descriptions of the steps involved in this analysis.\nData source: Abrahamson, W.G. 2019. Survival, growth and biomass estimates of two dominant palmetto species of south-central Florida from 1981 - 2017, ongoing at 5-year intervals ver 1. Environmental Data Initiative. https://doi.org/10.6073/pasta/f2f96ec76fbbd4b9db431c79a770c4d5\n\n\nObtain and Tidy Data\n\nLibraries\n\n\nCode\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(here)\nlibrary(cowplot)\nlibrary(knitr)\nlibrary(kableExtra)\n\n\n\n\nRead in data\n\n\nCode\np_df &lt;- read_csv(here(\"posts\", \"2024-03-01-blr\", \"data\", \"palmetto.csv\"))\n\n\n\n\nClean data\nI removed unnecessary columns and assigned factor levels to my two species ID numbers. Species 1 = S. repens and Species 2 = S. etonia\n\n\nCode\np_clean &lt;- p_df %&gt;% \n  select(species, height:green_lvs) %&gt;% \n  mutate(species = as_factor(species)) %&gt;% \n  drop_na()\n\n\n\n\n\nExploratory Visualizations\nI am interested in whether height, width, length, and number of green leaves are good variables to use to differentiate between my two species. To be good predictors, there should be notable differences in these values between the species. I decided to explore these four variables using boxplots.\n\n\nCode\nheight_plot &lt;- ggplot(data = p_clean) + \n  geom_boxplot(aes(x = as_factor(species), y = height), fill = \"olivedrab3\", color = \"olivedrab\") +\n  scale_x_discrete(labels = c(\"1\" = \"S. repens\", \"2\" = \"S. etonia\")) +\n  labs(x = \"\", y = \"Height (cm)\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(face = \"italic\"))\n\nwidth_plot &lt;- ggplot(data = p_clean) + \n  geom_boxplot(aes(x = as_factor(species), y = width), fill = \"olivedrab3\", color = \"olivedrab\") +\n  scale_x_discrete(labels = c(\"1\" = \"S. repens\", \"2\" = \"S. etonia\")) +\n  labs(x = \"\", y = \"Width (cm)\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(face = \"italic\"))\n\nlength_plot &lt;- ggplot(data = p_clean) + \n  geom_boxplot(aes(x = as_factor(species), y = length), fill = \"olivedrab3\", color = \"olivedrab\") +\n  scale_x_discrete(labels = c(\"1\" = \"S. repens\", \"2\" = \"S. etonia\")) +\n  labs(x = \"\", y = \"Length (cm)\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(face = \"italic\"))\n\nleaves_plot &lt;- ggplot(data = p_clean) + \n  geom_boxplot(aes(x = as_factor(species), y = green_lvs), fill = \"olivedrab3\", color = \"olivedrab\") +\n  scale_x_discrete(labels = c(\"1\" = \"S. repens\", \"2\" = \"S. etonia\")) +\n  labs(x = \"\", y = \"Number of Green Leaves\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(face = \"italic\"))\n\n\n\n\nCode\nplot_grid(height_plot, width_plot, length_plot, leaves_plot,\n          labels = c(\"A\", \"B\", \"C\", \"D\"),\n          ncol = 2,\n          vjust = 1)\n\n\n\n\n\nFigure 1: Boxplots comparing height, width, length, and number of green leaves for S. repens and S. etonia.\n\n\n\n\nTakeaways:\nPlant heights between the two species look fairly similar, so height may not be a good predictor of species. There may be some difference in plant widths, and even more noticeable differences in length and number of green leaves. I expect length and green leaves to be especially important predictors, since they seem to show the largest differences between species.\n\n\nBinary Logistic Regression Models\n\nCreate formulas\nI am interested in comparing two models. One model predicts species based on height, length, width, and green leaves (Model 1). The other model predicts species based only on height, width, and green leaves (Model 2). The difference between the two is whether length is used as a predictor variable.\n\n\nCode\n# model 1\nf1 &lt;- species ~ height + length + width + green_lvs\n# model 2\nf2 &lt;- species ~ height + width + green_lvs\n\n\n\n\nTen-Fold Cross Validation\nNow that the two model formulas are defined, it is time to see which performs better at classification. I decided to use ten-fold cross validation to repeatedly fit the models to ten different subsets of the data, and then I extracted the average performance metrics from each model.\n\n\nCode\n# create folded version of dataset\nset.seed(10101)\np_folds &lt;- vfold_cv(p_clean, v = 10, repeats = 10)\n\n\n\n\nCode\n# set up model\nblr_mdl &lt;- logistic_reg() %&gt;% \n  set_engine('glm')\n\n# create workflows\nblr_wf_1 &lt;- workflow() %&gt;% \n  add_model(blr_mdl) %&gt;% \n  add_formula(f1) \n\nblr_wf_2 &lt;- workflow() %&gt;% \n  add_model(blr_mdl) %&gt;% \n  add_formula(f2)\n\n# apply the workflows to the folded data\nblr_fit_folds_1 &lt;- blr_wf_1 %&gt;% \n  fit_resamples(p_folds)\nblr_fit_folds_2 &lt;- blr_wf_2 %&gt;% \n  fit_resamples(p_folds)\n\n\n\n\nCross Validation Results\n\n\nCode\ncollect_metrics(blr_fit_folds_1) %&gt;% \n  select(-.config) %&gt;% \n  rename(metric = .metric,\n         estimator = .estimator,\n         standard_error = std_err) %&gt;% \n  kbl() %&gt;% \n  kable_styling(\"basic\", position = \"center\")\ncollect_metrics(blr_fit_folds_2) %&gt;% \n  select(-.config) %&gt;% \n  rename(metric = .metric,\n         estimator = .estimator,\n         standard_error = std_err) %&gt;% \n  kbl() %&gt;% \n  kable_styling(\"basic\", position = \"center\")\n\n\n\nTable 1: Comparing the predictive performance of the two models. The results for the accuracy and area under the ROC curve are presented for both Model 1 (a) and Model 2 (b).\n\n\n\n\n(a) Model 1\n\n\nmetric\nestimator\nmean\nn\nstandard_error\n\n\n\n\naccuracy\nbinary\n0.9168905\n100\n0.0007788\n\n\nroc_auc\nbinary\n0.9725080\n100\n0.0003592\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Model 2\n\n\nmetric\nestimator\nmean\nn\nstandard_error\n\n\n\n\naccuracy\nbinary\n0.8988665\n100\n0.0008640\n\n\nroc_auc\nbinary\n0.9634580\n100\n0.0004167\n\n\n\n\n\n\n\n\n\n\n\nResults:\nBased on the results of the cross validation, I am choosing Model 1 as the better model. Model 1 has a greater area under the receiver operating characteristic (ROC) curve than Model 2 has (0.9725 compared with 0.9635). The model with a greater area under the curve is the better classifier. In addition, Model 1 had a slightly higher accuracy rate than Model 2 (91.7% compared with 89.9%). Though Model 1 ended up winning based on area under the curve and accuracy, it is worth noting that both models performed very well.\n\n\n\nTrain Selected Model\nI next trained Model 1 using the entire dataset (without any folding) to obtain my final coefficient results for each predictor variable (shown in the “estimate” column of the table below).\n\n\nCode\n## run model on entire dataset\nblr1_fit &lt;- blr_mdl %&gt;% \n  fit(formula = f1, data = p_clean)\n\n\n\n\nCode\n# create table with blr results\nbroom::tidy(blr1_fit) %&gt;% \n  select(-statistic) %&gt;% \n  kbl() %&gt;% \n  kable_styling(\"basic\", position = \"center\")\n\n\n\n\nTable 2: Final Model 1 results. Coefficient values for the intercept, as well as for each of the four predictor variables, are shown. Standard errors and p-values for the coefficients are also included.\n\n\nterm\nestimate\nstd.error\np.value\n\n\n\n\n(Intercept)\n3.2266851\n0.1420708\n0\n\n\nheight\n-0.0292173\n0.0023061\n0\n\n\nlength\n0.0458233\n0.0018661\n0\n\n\nwidth\n0.0394434\n0.0021000\n0\n\n\ngreen_lvs\n-1.9084747\n0.0388634\n0\n\n\n\n\n\n\n\n\n\n\nClassification Results\nUsing my finalized Model 1, I decided to evaluate its predictive strength. I generated predicted species classifications for each observed set of plant height, width, length, and green leaves in the dataset, then compared these predictions to the actual species identities. Predictions were made using a 50% cutoff, meaning that a species was classified as species 1 if the probability of it being species 1, based on my Model 1 results, was 50% or greater.\n\n\nCode\n# generate predictions\np_predict &lt;- p_clean %&gt;% \n  mutate(predict(blr1_fit, new_data = .)) %&gt;% \n  mutate(predict(blr1_fit, new_data = ., type = 'prob'))\n\n# make table of prediction results\npredict_table &lt;- table(p_predict %&gt;%\n        select(species, .pred_class))\n\n\n\n\nCode\nkbl(data.frame(\n  species = c(\"1\", \"2\"),\n  n_correct = c(5548, 5701),\n  n_incorrect = c(564, 454)) %&gt;% \n  mutate(p_correct = n_correct/(n_correct+n_incorrect))) %&gt;% \n  kable_styling(\"basic\", position = \"center\")\n\n\n\n\nTable 3: Model 1 predictions. For each species, the number of correct predictions and incorrect predictions are shown. The leftmost column depicts the percent of predictions that were correct.\n\n\nspecies\nn_correct\nn_incorrect\np_correct\n\n\n\n\n1\n5548\n564\n0.9077225\n\n\n2\n5701\n454\n0.9262388\n\n\n\n\n\n\n\n\nConclusion:\nMy fitted Model 1 classified observations of species 1 (S. repens) correctly 91% of the time, and of species 2 (S. etonia) correctly 93% of the time. While the model is not perfect, it does have a very high accuracy rate. The model could potentially be improved if other strong predictor variables could be found and included."
  },
  {
    "objectID": "posts/2024-02-17-text-analysis/index.html",
    "href": "posts/2024-02-17-text-analysis/index.html",
    "title": "Text Sentiment Analysis",
    "section": "",
    "text": "This analysis uses the text from the book “Harry Potter and the Prisoner of Azkaban” by J.K. Rowling. The story follows Harry through his third year at Hogwarts, as he learns to fight Dementors, sneak around using the Marauder’s Map, and even travel in time! By the end of the story, Harry learns a lot about the mysterious escaped criminal Sirius Black and about his own parents’ past.\nSource: Rowling, J. K. Harry Potter and the Prisoner of Azkaban. New York: Arthur A. Levine Books, 1999. Full text here.\n\n\n\nThis analysis had two main goals:\n\nFind and visualize the most common words within each chapter and in the book as a whole\nPerform a sentiment analysis to visualize the tone (positive or negative) of each chapter of the book\n\n\n\n\n\nImport & Tidy Text\n\nPrepare the data\n\nRead in the complete PDF of Harry Potter and the Prisoner of Azkaban\nAdd column for page number\nCreate character strings for each line of text in the whole book\nLet each line of text be its own row in the dataframe\nRemove extra whitespaces\nAdd column for chapter number (in numeric format)\n\nExtract every individual word\n\nLet each word in the text be its own row in the dataframe (associated with the page and chapter from which it came)\n\nRemove stop words\n\nStop words are commonly used words that don’t carry significant meaning (e.g., “of”, “a”, “the”, “and”)\nRemove stop words from this dataset\n\n\nFind Most-Used Words\n\nCalculate the five most-used words in each chapter and visualize them\nGraph the frequency of different character names being mentioned throughout the book\nFind the top 100 most-used words in the entire book and visualize them as a word cloud\n\nPerform Sentiment Analysis\n\nUse the “afinn” lexicon to assign each word a value on the positive/negative scale\n\n-5 being most negative, 5 being most positive\n\nFor each chapter, take a weighted average of the positivity/negativity scores (weighted by the amount of times each word was used)\nVisualize how the tone of the book changes in each chapter\n\n\n\n\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(here)\nlibrary(tidytext)\nlibrary(pdftools)\nlibrary(ggwordcloud)\nlibrary(textdata)\n\n\n\n\n\n\nCode\n### Read in\nhp3_text &lt;- pdftools::pdf_text(here('posts', '2024-02-17-text-analysis', 'data', 'hp_3.pdf'))\n\n\n\n\nCode\n### Wrangle and tidy\nhp3_lines &lt;- data.frame(hp3_text) %&gt;% \n  mutate(page = 1:n() - 1) %&gt;% \n  mutate(text_full = str_split(hp3_text, pattern = '\\\\n')) %&gt;%  # creates character strings of each line\n  unnest(text_full) %&gt;%  # make row for each line of text\n  select(!hp3_text) %&gt;% # don't need original data column anymore\n  mutate(text_full = str_squish(text_full)) # remove any extra whitespace\n\n\n\n\nCode\n### Add chapters in separate column\nhp3_chapters &lt;- hp3_lines %&gt;% \n  slice(-1) %&gt;% # remove empty first row\n  mutate(chapter = ifelse(str_detect(text_full, \"CHAPTER\"), text_full, NA)) %&gt;% # creates new chapter column\n  mutate(chapter = str_remove(chapter, \"CHAPTER\")) %&gt;% # remove the word \"chapter\"\n  fill(chapter, .direction = 'down') %&gt;% # assign chapter to all entries in each chapter\n  mutate(chapter_num = case_when(\n    chapter == \" ONE\" ~ 1,\n    chapter == \" TWO\" ~ 2,\n    chapter == \" THREE\" ~ 3,\n    chapter == \" FOUR\" ~ 4,\n    chapter == \" FIVE\" ~ 5,\n    chapter == \" SIX\" ~ 6,\n    chapter == \" SEVEN\" ~ 7,\n    chapter == \" EIGHT\" ~ 8,\n    chapter == \" NINE\" ~ 9,\n    chapter == \" TEN\" ~ 10,\n    chapter == \" ELEVEN\" ~ 11,\n    chapter == \" TWELVE\" ~ 12,\n    chapter == \" THIRTEEN\" ~ 13,\n    chapter == \" FOURTEEN\" ~ 14,\n    chapter == \" FIFTEEN\" ~ 15,\n    chapter == \" SIXTEEN\" ~ 16,\n    chapter == \" SEVENTEEN\" ~ 17,\n    chapter == \" EIGHTEEN\" ~ 18,\n    chapter == \" NINETEEN\" ~ 19,\n    chapter == \" TWENTY\" ~ 20,\n    chapter == \" TWENTY-ONE\" ~ 21,\n    chapter == \" TWENTY-TWO\" ~ 22\n    )) # change written chapter numbers into actual numbers\n\n\n\n\n\n\n\nCode\n### Get words\nhp3_words &lt;- hp3_chapters %&gt;% \n  unnest_tokens(word, text_full) %&gt;% \n  select(page, chapter_num, word) %&gt;% \n  mutate(word = str_split_i(word, pattern = \"'s\", 1)) # some have 's, (harry's), want this to count with root word\n\n### Wordcount\nhp3_wordcount &lt;- hp3_words %&gt;% \n  count(chapter_num, word)\n\n\n\n\n\n\n\nCode\nhp3_wordcount_clean &lt;- hp3_wordcount %&gt;% \n  anti_join(stop_words, by = \"word\") \n\n\n\n\n\n\n\n\n\n\nCode\n### Top 5 words for each chapter\ntop_5_words &lt;- hp3_wordcount_clean %&gt;% \n  group_by(chapter_num) %&gt;% \n  arrange(-n) %&gt;% \n  slice(1:5) %&gt;% \n  ungroup()\n\n### Plot\nggplot(top_5_words, aes(x = n, y = word)) +\n  geom_col(fill = \"#740001\") +\n  facet_wrap(~as.factor(chapter_num), scales = \"free\") +\n  labs(x = \"\", y = \"\") +\n  theme_minimal()\n\n\n\n\n\nFigure 1: The top 5 words used in each chapter in the book. Bar sizes depict the amount of times each word was used.\n\n\n\n\nMany of the most used words are the names of characters, which makes sense given it’s a book with a lot of dialogue and third-person narration. Using character names as a proxy for their relevance in any given chapter, we can track how certain characters appear / disappear from the narrative:\n\n\nCode\n### Look at some key characters over the course of the book\nhp3_character_count &lt;- hp3_wordcount_clean %&gt;% \n  filter(word %in% c(\"hagrid\", \"lupin\", \"buckbeak\", \"snape\", \"pettigrew\", \"sirius\"))\n\nhp3_words_by_chap &lt;- hp3_wordcount_clean %&gt;% \n  group_by(chapter_num) %&gt;% \n  summarize(word_count = sum(n))\n\nhp3_characters_by_chap &lt;- left_join(hp3_character_count, hp3_words_by_chap, by = \"chapter_num\") %&gt;% \n  mutate(freq_per_chap = n/word_count)\n\n### Plot\nggplot(hp3_characters_by_chap, aes(x = as.factor(chapter_num), y = freq_per_chap, color = word, group = word)) +\n  geom_point() +\n  geom_line() +\n  labs(x = \"\", y = \"Frequency\") +\n  facet_wrap(~word, scales = \"free_y\", nrow = 3) +\n  scale_color_manual(values = c(\"#740001\", \"#AE0001\", \"#EEBA30\", \"#D3A625\", \"#000000\", \"darkgreen\")) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\nFigure 2: The frequency of Buckbeak, Hagrid, Lupin, Pettigrew, Sirius, and Snape being mentioned in each chapter of the book. The frequency was calculated by dividing the number of uses of each word by the total number of words in that chapter.\n\n\n\n\n\n\n\n\n\nCode\n### Top 100 words from whole book\nhp3_top100 &lt;- hp3_wordcount_clean %&gt;% \n  group_by(word) %&gt;% \n  summarize(n = sum(n)) %&gt;% \n  arrange(-n) %&gt;% \n  slice(1:100)\n\n\n\n\nCode\n### Create wordcloud\nggplot(data = hp3_top100, aes(label = word)) +\n  geom_text_wordcloud(aes(color = n, size = n+500), shape = \"star\") +\n  scale_size_area(max_size = 6) +\n  scale_color_gradientn(colors = c(\"#740001\",\"#D3A625\",\"#AE0001\")) +\n  theme_minimal()\n\n\n\n\n\nFigure 3: Wordcloud of the top 100 words used in the book, sized by their number of uses.\n\n\n\n\n\n\n\n\n\n\nCode\n### Read in the 'afinn' lexicon\nafinn_lex &lt;- get_sentiments(lexicon = \"afinn\")\n\n\n\n\nCode\n### Join the lexicon to our words\nhp3_afinn &lt;- hp3_words %&gt;% \n  inner_join(afinn_lex, by = 'word')\n\n\n\n\nCode\n### Count the number of words in each chapter assigned to each value (from -5 to 5)\nafinn_counts &lt;- hp3_afinn %&gt;% \n  group_by(chapter_num, value) %&gt;%\n  summarize(n = n())\n\n### Take a weighted average of the values (using the number of words to weight)\nafinn_mean &lt;- afinn_counts %&gt;% \n  summarize(weighted_avg_value = weighted.mean(value, n))\n\n### Plot \nggplot(data = afinn_mean) +\n  geom_col(aes(x = as.factor(chapter_num), y = weighted_avg_value, fill = weighted_avg_value &gt; 0)) +\n  labs(x = \"Chapter\", y = \"Average Word Positivity\") +\n  scale_fill_manual(name = 'Positive?', values = setNames(c('#D3A625','#AE0001'), c(T, F))) +\n  theme_minimal() +\n  theme(legend.position = \"none\") \n\n\n\n\n\nFigure 4: Sentiment analysis of the average positivity of each chapter in the book. Values above 0 indicate the chapter had an overall positive tone. Values below 0 indicate a negative tone. Average positivity was calculated by weighting the positivity score of words by their amount of useage.\n\n\n\n\nThe first portion of the book has some overall positive chapters (like Chapter 4, where Harry returns to school and reunites with his friends) and some more negative chapters (like Chapters 2 and 3, where Harry accidentally inflates his Aunt Marge like a balloon, and then must run away and catch the chaotic Knight Bus). The latter half of the book takes on a much heavier and more negative tone, with the most negative chapter being Chapter 17, where Harry, Ron, and Hermione find themselves caught in the Shrieking Shack amidst a showdown between Sirius Black, Professor Lupin, Professor Snape, and Peter Pettigrew (as the rat Scabbers). The book ultimately ends on a positive note in the final chapter, once Sirius and Buckbeak have safely escaped from their respective death sentences!"
  },
  {
    "objectID": "posts/2024-02-17-text-analysis/index.html#text-analysis-overview",
    "href": "posts/2024-02-17-text-analysis/index.html#text-analysis-overview",
    "title": "Text Sentiment Analysis",
    "section": "",
    "text": "This analysis uses the text from the book “Harry Potter and the Prisoner of Azkaban” by J.K. Rowling. The story follows Harry through his third year at Hogwarts, as he learns to fight Dementors, sneak around using the Marauder’s Map, and even travel in time! By the end of the story, Harry learns a lot about the mysterious escaped criminal Sirius Black and about his own parents’ past.\nSource: Rowling, J. K. Harry Potter and the Prisoner of Azkaban. New York: Arthur A. Levine Books, 1999. Full text here.\n\n\n\nThis analysis had two main goals:\n\nFind and visualize the most common words within each chapter and in the book as a whole\nPerform a sentiment analysis to visualize the tone (positive or negative) of each chapter of the book\n\n\n\n\n\nImport & Tidy Text\n\nPrepare the data\n\nRead in the complete PDF of Harry Potter and the Prisoner of Azkaban\nAdd column for page number\nCreate character strings for each line of text in the whole book\nLet each line of text be its own row in the dataframe\nRemove extra whitespaces\nAdd column for chapter number (in numeric format)\n\nExtract every individual word\n\nLet each word in the text be its own row in the dataframe (associated with the page and chapter from which it came)\n\nRemove stop words\n\nStop words are commonly used words that don’t carry significant meaning (e.g., “of”, “a”, “the”, “and”)\nRemove stop words from this dataset\n\n\nFind Most-Used Words\n\nCalculate the five most-used words in each chapter and visualize them\nGraph the frequency of different character names being mentioned throughout the book\nFind the top 100 most-used words in the entire book and visualize them as a word cloud\n\nPerform Sentiment Analysis\n\nUse the “afinn” lexicon to assign each word a value on the positive/negative scale\n\n-5 being most negative, 5 being most positive\n\nFor each chapter, take a weighted average of the positivity/negativity scores (weighted by the amount of times each word was used)\nVisualize how the tone of the book changes in each chapter"
  },
  {
    "objectID": "posts/2024-02-17-text-analysis/index.html#import-tidy-text",
    "href": "posts/2024-02-17-text-analysis/index.html#import-tidy-text",
    "title": "Text Sentiment Analysis",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(here)\nlibrary(tidytext)\nlibrary(pdftools)\nlibrary(ggwordcloud)\nlibrary(textdata)\n\n\n\n\n\n\nCode\n### Read in\nhp3_text &lt;- pdftools::pdf_text(here('posts', '2024-02-17-text-analysis', 'data', 'hp_3.pdf'))\n\n\n\n\nCode\n### Wrangle and tidy\nhp3_lines &lt;- data.frame(hp3_text) %&gt;% \n  mutate(page = 1:n() - 1) %&gt;% \n  mutate(text_full = str_split(hp3_text, pattern = '\\\\n')) %&gt;%  # creates character strings of each line\n  unnest(text_full) %&gt;%  # make row for each line of text\n  select(!hp3_text) %&gt;% # don't need original data column anymore\n  mutate(text_full = str_squish(text_full)) # remove any extra whitespace\n\n\n\n\nCode\n### Add chapters in separate column\nhp3_chapters &lt;- hp3_lines %&gt;% \n  slice(-1) %&gt;% # remove empty first row\n  mutate(chapter = ifelse(str_detect(text_full, \"CHAPTER\"), text_full, NA)) %&gt;% # creates new chapter column\n  mutate(chapter = str_remove(chapter, \"CHAPTER\")) %&gt;% # remove the word \"chapter\"\n  fill(chapter, .direction = 'down') %&gt;% # assign chapter to all entries in each chapter\n  mutate(chapter_num = case_when(\n    chapter == \" ONE\" ~ 1,\n    chapter == \" TWO\" ~ 2,\n    chapter == \" THREE\" ~ 3,\n    chapter == \" FOUR\" ~ 4,\n    chapter == \" FIVE\" ~ 5,\n    chapter == \" SIX\" ~ 6,\n    chapter == \" SEVEN\" ~ 7,\n    chapter == \" EIGHT\" ~ 8,\n    chapter == \" NINE\" ~ 9,\n    chapter == \" TEN\" ~ 10,\n    chapter == \" ELEVEN\" ~ 11,\n    chapter == \" TWELVE\" ~ 12,\n    chapter == \" THIRTEEN\" ~ 13,\n    chapter == \" FOURTEEN\" ~ 14,\n    chapter == \" FIFTEEN\" ~ 15,\n    chapter == \" SIXTEEN\" ~ 16,\n    chapter == \" SEVENTEEN\" ~ 17,\n    chapter == \" EIGHTEEN\" ~ 18,\n    chapter == \" NINETEEN\" ~ 19,\n    chapter == \" TWENTY\" ~ 20,\n    chapter == \" TWENTY-ONE\" ~ 21,\n    chapter == \" TWENTY-TWO\" ~ 22\n    )) # change written chapter numbers into actual numbers\n\n\n\n\n\n\n\nCode\n### Get words\nhp3_words &lt;- hp3_chapters %&gt;% \n  unnest_tokens(word, text_full) %&gt;% \n  select(page, chapter_num, word) %&gt;% \n  mutate(word = str_split_i(word, pattern = \"'s\", 1)) # some have 's, (harry's), want this to count with root word\n\n### Wordcount\nhp3_wordcount &lt;- hp3_words %&gt;% \n  count(chapter_num, word)\n\n\n\n\n\n\n\nCode\nhp3_wordcount_clean &lt;- hp3_wordcount %&gt;% \n  anti_join(stop_words, by = \"word\")"
  },
  {
    "objectID": "posts/2024-02-17-text-analysis/index.html#most-used-words",
    "href": "posts/2024-02-17-text-analysis/index.html#most-used-words",
    "title": "Text Sentiment Analysis",
    "section": "",
    "text": "Code\n### Top 5 words for each chapter\ntop_5_words &lt;- hp3_wordcount_clean %&gt;% \n  group_by(chapter_num) %&gt;% \n  arrange(-n) %&gt;% \n  slice(1:5) %&gt;% \n  ungroup()\n\n### Plot\nggplot(top_5_words, aes(x = n, y = word)) +\n  geom_col(fill = \"#740001\") +\n  facet_wrap(~as.factor(chapter_num), scales = \"free\") +\n  labs(x = \"\", y = \"\") +\n  theme_minimal()\n\n\n\n\n\nFigure 1: The top 5 words used in each chapter in the book. Bar sizes depict the amount of times each word was used.\n\n\n\n\nMany of the most used words are the names of characters, which makes sense given it’s a book with a lot of dialogue and third-person narration. Using character names as a proxy for their relevance in any given chapter, we can track how certain characters appear / disappear from the narrative:\n\n\nCode\n### Look at some key characters over the course of the book\nhp3_character_count &lt;- hp3_wordcount_clean %&gt;% \n  filter(word %in% c(\"hagrid\", \"lupin\", \"buckbeak\", \"snape\", \"pettigrew\", \"sirius\"))\n\nhp3_words_by_chap &lt;- hp3_wordcount_clean %&gt;% \n  group_by(chapter_num) %&gt;% \n  summarize(word_count = sum(n))\n\nhp3_characters_by_chap &lt;- left_join(hp3_character_count, hp3_words_by_chap, by = \"chapter_num\") %&gt;% \n  mutate(freq_per_chap = n/word_count)\n\n### Plot\nggplot(hp3_characters_by_chap, aes(x = as.factor(chapter_num), y = freq_per_chap, color = word, group = word)) +\n  geom_point() +\n  geom_line() +\n  labs(x = \"\", y = \"Frequency\") +\n  facet_wrap(~word, scales = \"free_y\", nrow = 3) +\n  scale_color_manual(values = c(\"#740001\", \"#AE0001\", \"#EEBA30\", \"#D3A625\", \"#000000\", \"darkgreen\")) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\nFigure 2: The frequency of Buckbeak, Hagrid, Lupin, Pettigrew, Sirius, and Snape being mentioned in each chapter of the book. The frequency was calculated by dividing the number of uses of each word by the total number of words in that chapter.\n\n\n\n\n\n\n\n\n\nCode\n### Top 100 words from whole book\nhp3_top100 &lt;- hp3_wordcount_clean %&gt;% \n  group_by(word) %&gt;% \n  summarize(n = sum(n)) %&gt;% \n  arrange(-n) %&gt;% \n  slice(1:100)\n\n\n\n\nCode\n### Create wordcloud\nggplot(data = hp3_top100, aes(label = word)) +\n  geom_text_wordcloud(aes(color = n, size = n+500), shape = \"star\") +\n  scale_size_area(max_size = 6) +\n  scale_color_gradientn(colors = c(\"#740001\",\"#D3A625\",\"#AE0001\")) +\n  theme_minimal()\n\n\n\n\n\nFigure 3: Wordcloud of the top 100 words used in the book, sized by their number of uses."
  },
  {
    "objectID": "posts/2024-02-17-text-analysis/index.html#sentiment-analysis",
    "href": "posts/2024-02-17-text-analysis/index.html#sentiment-analysis",
    "title": "Text Sentiment Analysis",
    "section": "",
    "text": "Code\n### Read in the 'afinn' lexicon\nafinn_lex &lt;- get_sentiments(lexicon = \"afinn\")\n\n\n\n\nCode\n### Join the lexicon to our words\nhp3_afinn &lt;- hp3_words %&gt;% \n  inner_join(afinn_lex, by = 'word')\n\n\n\n\nCode\n### Count the number of words in each chapter assigned to each value (from -5 to 5)\nafinn_counts &lt;- hp3_afinn %&gt;% \n  group_by(chapter_num, value) %&gt;%\n  summarize(n = n())\n\n### Take a weighted average of the values (using the number of words to weight)\nafinn_mean &lt;- afinn_counts %&gt;% \n  summarize(weighted_avg_value = weighted.mean(value, n))\n\n### Plot \nggplot(data = afinn_mean) +\n  geom_col(aes(x = as.factor(chapter_num), y = weighted_avg_value, fill = weighted_avg_value &gt; 0)) +\n  labs(x = \"Chapter\", y = \"Average Word Positivity\") +\n  scale_fill_manual(name = 'Positive?', values = setNames(c('#D3A625','#AE0001'), c(T, F))) +\n  theme_minimal() +\n  theme(legend.position = \"none\") \n\n\n\n\n\nFigure 4: Sentiment analysis of the average positivity of each chapter in the book. Values above 0 indicate the chapter had an overall positive tone. Values below 0 indicate a negative tone. Average positivity was calculated by weighting the positivity score of words by their amount of useage.\n\n\n\n\nThe first portion of the book has some overall positive chapters (like Chapter 4, where Harry returns to school and reunites with his friends) and some more negative chapters (like Chapters 2 and 3, where Harry accidentally inflates his Aunt Marge like a balloon, and then must run away and catch the chaotic Knight Bus). The latter half of the book takes on a much heavier and more negative tone, with the most negative chapter being Chapter 17, where Harry, Ron, and Hermione find themselves caught in the Shrieking Shack amidst a showdown between Sirius Black, Professor Lupin, Professor Snape, and Peter Pettigrew (as the rat Scabbers). The book ultimately ends on a positive note in the final chapter, once Sirius and Buckbeak have safely escaped from their respective death sentences!"
  },
  {
    "objectID": "posts/2024-03-02-nls/index.html",
    "href": "posts/2024-03-02-nls/index.html",
    "title": "Non-Linear Least Squares",
    "section": "",
    "text": "Description:\nThe data used in this analysis is from a study of sorghum and maize growth in Greece. Sweet sorghum, fiber sorghum, and maize were grown in experimental plots. Half of the plots for each plant were exposed to optimum inputs of water and nitrogen, while the other half were exposed to limited water and nitrogen inputs. The scientists measured how the biomass (yield) for each crop type changed over time under these different experimental conditions.\nVariables:\n\nDOY = day of year (141 - 303)\nBlock = block in experimental design (1 - 4)\nInput = level of water and nitrogen input. Either 1 (low) or 2 (high)\nCrop = crop type (M = maize, S = sweet sorghum, F = fiber sorghum)\nYield = biomass yield in Mg/ha\n\nData source:\nDanalatos, N.G., S.V. Archontoulis, and K. Tsiboukas. 2009. Comparative analysis of sorghum versus corn growing under optimum and under water/nitrogen limited conditions in central Greece. In: From research to industry and markets: Proceedings of the 17th European Biomass Conference, Hamburg, Germany. 29 June–3 July 2009. ETA–Renewable Energies, Florence, Italy. p. 538–544.\n\n\n\nThe purpose of this analysis is to fit models to the crop dataset using non-linear least squares (NLS). I can optimize the parameters of the model to find the best fitted model for each species of crop. These models can then be used to predict crop yields at various time points for each of these crops, and to assess the impact of fertilizer use on overall crop growth.\nThe Beta function\nThe model for this analysis is the Beta function from Table 1, Equation 2.5 of Archontoulis & Miguez, 2015 (see here). This is a sigmoid function and I will use it to describe plant biomass (yield) as a function of time.\nBeta function variables and parameters\n\ny = response variable\nymax = maximum value of response variable\nt = explanatory variable\nte = value of explanatory variable when y equals its asymptotic value\ntm = inflection point at which growth rate is at its maximum\n\n\n\nThe steps of this analysis are as follows:\n\nWrite Beta function\n\nRead in and tidy data\nCreate a function in R based upon the Beta function’s mathematical formula\n\nRun one NLS model\n\nGenerate initial guesses for parameter values\nRun model for just one crop and input level\nPlot fitted model\n\nRun multiple NLS models\n\nRun models on each combination of plot, crop species, and input level\nFind the best fitted model for each crop\n\nCreate final visual\n\nPlot crop data with smoothed models to understand how fertilizer impacts yield"
  },
  {
    "objectID": "posts/2024-03-02-nls/index.html#data-summary",
    "href": "posts/2024-03-02-nls/index.html#data-summary",
    "title": "Non-Linear Least Squares",
    "section": "",
    "text": "Description:\nThe data used in this analysis is from a study of sorghum and maize growth in Greece. Sweet sorghum, fiber sorghum, and maize were grown in experimental plots. Half of the plots for each plant were exposed to optimum inputs of water and nitrogen, while the other half were exposed to limited water and nitrogen inputs. The scientists measured how the biomass (yield) for each crop type changed over time under these different experimental conditions.\nVariables:\n\nDOY = day of year (141 - 303)\nBlock = block in experimental design (1 - 4)\nInput = level of water and nitrogen input. Either 1 (low) or 2 (high)\nCrop = crop type (M = maize, S = sweet sorghum, F = fiber sorghum)\nYield = biomass yield in Mg/ha\n\nData source:\nDanalatos, N.G., S.V. Archontoulis, and K. Tsiboukas. 2009. Comparative analysis of sorghum versus corn growing under optimum and under water/nitrogen limited conditions in central Greece. In: From research to industry and markets: Proceedings of the 17th European Biomass Conference, Hamburg, Germany. 29 June–3 July 2009. ETA–Renewable Energies, Florence, Italy. p. 538–544."
  },
  {
    "objectID": "posts/2024-03-02-nls/index.html#purpose",
    "href": "posts/2024-03-02-nls/index.html#purpose",
    "title": "Non-Linear Least Squares",
    "section": "",
    "text": "The purpose of this analysis is to fit models to the crop dataset using non-linear least squares (NLS). I can optimize the parameters of the model to find the best fitted model for each species of crop. These models can then be used to predict crop yields at various time points for each of these crops, and to assess the impact of fertilizer use on overall crop growth.\nThe Beta function\nThe model for this analysis is the Beta function from Table 1, Equation 2.5 of Archontoulis & Miguez, 2015 (see here). This is a sigmoid function and I will use it to describe plant biomass (yield) as a function of time.\nBeta function variables and parameters\n\ny = response variable\nymax = maximum value of response variable\nt = explanatory variable\nte = value of explanatory variable when y equals its asymptotic value\ntm = inflection point at which growth rate is at its maximum\n\n\n\nThe steps of this analysis are as follows:\n\nWrite Beta function\n\nRead in and tidy data\nCreate a function in R based upon the Beta function’s mathematical formula\n\nRun one NLS model\n\nGenerate initial guesses for parameter values\nRun model for just one crop and input level\nPlot fitted model\n\nRun multiple NLS models\n\nRun models on each combination of plot, crop species, and input level\nFind the best fitted model for each crop\n\nCreate final visual\n\nPlot crop data with smoothed models to understand how fertilizer impacts yield"
  },
  {
    "objectID": "posts/2024-03-16-pca/index.html",
    "href": "posts/2024-03-16-pca/index.html",
    "title": "Principal Components Analysis",
    "section": "",
    "text": "Principal components analysis (PCA) is used to simplify multivariate data into just two dimensions while explaining as much of the variance as possible. In this data set, each data point is one census tract and its associated environmental health variables, as well as its historically assigned redlining grade. Historic redlining was largely based upon racial demographics of different neighborhoods. Since redlining grades were used by mortgage lenders to make home loan and investment decisions, neighborhoods in lower grades systematically received less investments and resources. Therefore, the legacies of redlining may have disproportionately negative impacts on communities of color today.\nI am interested in using this analysis to determine whether historical redlining is related to modern-day patterns in urban health inequalities. I will conduct PCA to examine how these health and environmental variables are related to each other, and whether census tracts from distinct redlining categories are dissimilar to each other.\n\n\n\nThe data used in this analysis describe each census tract in the City of Los Angeles in terms of various health indicators and environmental characteristics. Specifically, I am interested in examining the relationships between PM2.5 pollution, diesel pollution, lead paint exposure, drinking water quality, asthma, education, poverty, unemployment, tree canopy coverage, and heat related illness rates. In addition, where data on historic redlining is available, each census tract is matched to its historic redlining grade (A = Best, B = Still Desirable, C = Definitely Declining, D = Hazardous).\nData sources:\n\nCensus/Health Data: California Office of Environmental Health Hazard Assessment. “CalEnviroScreen 4.0” https://oehha.ca.gov/calenviroscreen/maps-data/download-data. February 2024.\nCanopy Coverage Data: Loyola Marymount University. “Tree Canopy Coverage”. March 2024.\nModern Redlining Data: diversitydatakids.org. 2023. “Home Owner Loan Corporation (HOLC) neighborhood grades for US census”. https://data.diversitydatakids.org/dataset/holc_census_tracts-home-owner-loan-corporation--holc--neighborhood-grades-for-us-census-tracts?_external=True on Mar 03 2024, based on HOLC maps digitized by the Digital Scholarship Lab at the University of Richmond.\nHeat Risk Data: UCLA Center for Healthy Climate Solutions & UCLA Center for Public Health and Disasters. “UCLA Heat Maps”. https://sites.google.com/g.ucla.edu/uclaheatmaps/map. Downloaded February 2024.\n\n\n\n\nTo conduct this PCA analysis, I used the following process:\n\nGet and tidy data\nExamine variable distributions and log variables if needed\nRun PCA\nPlot results"
  },
  {
    "objectID": "posts/2024-03-16-pca/index.html#purpose",
    "href": "posts/2024-03-16-pca/index.html#purpose",
    "title": "Principal Components Analysis",
    "section": "",
    "text": "Principal components analysis (PCA) is used to simplify multivariate data into just two dimensions while explaining as much of the variance as possible. In this data set, each data point is one census tract and its associated environmental health variables, as well as its historically assigned redlining grade. Historic redlining was largely based upon racial demographics of different neighborhoods. Since redlining grades were used by mortgage lenders to make home loan and investment decisions, neighborhoods in lower grades systematically received less investments and resources. Therefore, the legacies of redlining may have disproportionately negative impacts on communities of color today.\nI am interested in using this analysis to determine whether historical redlining is related to modern-day patterns in urban health inequalities. I will conduct PCA to examine how these health and environmental variables are related to each other, and whether census tracts from distinct redlining categories are dissimilar to each other."
  },
  {
    "objectID": "posts/2024-03-16-pca/index.html#data-summary",
    "href": "posts/2024-03-16-pca/index.html#data-summary",
    "title": "Principal Components Analysis",
    "section": "",
    "text": "The data used in this analysis describe each census tract in the City of Los Angeles in terms of various health indicators and environmental characteristics. Specifically, I am interested in examining the relationships between PM2.5 pollution, diesel pollution, lead paint exposure, drinking water quality, asthma, education, poverty, unemployment, tree canopy coverage, and heat related illness rates. In addition, where data on historic redlining is available, each census tract is matched to its historic redlining grade (A = Best, B = Still Desirable, C = Definitely Declining, D = Hazardous).\nData sources:\n\nCensus/Health Data: California Office of Environmental Health Hazard Assessment. “CalEnviroScreen 4.0” https://oehha.ca.gov/calenviroscreen/maps-data/download-data. February 2024.\nCanopy Coverage Data: Loyola Marymount University. “Tree Canopy Coverage”. March 2024.\nModern Redlining Data: diversitydatakids.org. 2023. “Home Owner Loan Corporation (HOLC) neighborhood grades for US census”. https://data.diversitydatakids.org/dataset/holc_census_tracts-home-owner-loan-corporation--holc--neighborhood-grades-for-us-census-tracts?_external=True on Mar 03 2024, based on HOLC maps digitized by the Digital Scholarship Lab at the University of Richmond.\nHeat Risk Data: UCLA Center for Healthy Climate Solutions & UCLA Center for Public Health and Disasters. “UCLA Heat Maps”. https://sites.google.com/g.ucla.edu/uclaheatmaps/map. Downloaded February 2024."
  },
  {
    "objectID": "posts/2024-03-16-pca/index.html#analytical-outline",
    "href": "posts/2024-03-16-pca/index.html#analytical-outline",
    "title": "Principal Components Analysis",
    "section": "",
    "text": "To conduct this PCA analysis, I used the following process:\n\nGet and tidy data\nExamine variable distributions and log variables if needed\nRun PCA\nPlot results"
  },
  {
    "objectID": "posts/2024-02-16-spatial-data/index.html",
    "href": "posts/2024-02-16-spatial-data/index.html",
    "title": "Spatial Data Analysis",
    "section": "",
    "text": "Oil spill cleanup at a beach in Orange County, CA. Photo credits: New York Magazine\n\n\n\n\n\n\nThis analysis looks at inland oil spills across the state of California in 2008, as documented by the California Department of Fish and Wildlife Office of Spill Prevention and Response (OSPR).\nData source: California Department of Fish and Wildlife. Oil Spill Incident Tracking. Published Jul 29 2009. Last updated Oct 24 2023. Data download available here.\n\n\n\nThis analysis had three main goals:\n\nVisualize the locations of 2008 oil spills across the state of California\nIdentify which counties in the state had the highest number of oil spills that year\nAssess whether oil spills are spatially clustered or randomly spaced across the state\n\n\n\n\n\nImport and Clean Data\n\nRead in California counties shapefile\nRead in CSV file containing oil spill data\nConvert oil spill dataframe to simple features object\nCheck the CRS of the counties file; set oil spill sf to same CRS\n\nCreate Interactive Map\n\nCreate map of California with points denoting oil spills\nMake map interactive so the user can zoom and click on points\n\nCreate Choropleth Map\n\nSpatial join the counties with the oil spill points\nCalculate the number of oil spills in each county\nVisualize on a static choropleth map to identify counties with highest oil spill incidences\n\nPoint Pattern Analysis\n\nConvert oil spill observations into a spatial point pattern\nUse the state of California as our observation window\nCalculate actual and theoretical (complete spatial randomness) nearest neighbor distances using the G function\nPlot the G function results for our observed data and the theoretical data\n\n\n\n\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(here)\nlibrary(sf)\nlibrary(tmap)\nlibrary(spatstat)\n\n\n\n\n\n\nCode\n### Read in California counties\nca_counties_sf &lt;- read_sf(here('posts', '2024-02-16-spatial-data', 'data', 'ca_counties'), layer = 'CA_Counties_TIGER2016') %&gt;%\n  janitor::clean_names() %&gt;% \n  select(name)\n\n### Read in oil spill csv\noil_df &lt;- read_csv(here('posts', '2024-02-16-spatial-data', 'data', 'oil_spill.csv')) %&gt;% \n  janitor::clean_names()\n\n\n\n\n\n\n\nCode\n### Convert lat-long oil dataframe to sf\noil_sf &lt;- oil_df %&gt;% \n  drop_na(x, y) %&gt;% \n  st_as_sf(coords = c(\"x\", \"y\"))\n\n\n\n\n\n\n\nCode\n### Check CRS of counties file\n# st_crs(ca_counties_sf) # \"EPSG\", 3857\n\n### Set oil sf CRS to CRS of CA counties\nst_crs(oil_sf) &lt;- 3857\n\n### Check CRSs are equal\n# st_crs(oil_sf) == st_crs(ca_counties_sf)\n\n\n\n\n\n\n\n\nCode\n### set the viewing mode to interactive\ntmap_mode(mode = 'plot')\n\ntm_shape(ca_counties_sf) +\n  tm_fill(col = \"white\") +\n  tm_shape(oil_sf) +\n  tm_dots(col = \"darkblue\")\n\n\n\n\n\nFigure 1: Oil spill incidents across California in 2008. Clicking a point will reveal its incident date, location, the affected waterway, and other key information.\n\n\n\n\n\n\n\n\n\nCode\n### Spatial join counties and oil spills\ncounties_oil_sf &lt;- st_join(ca_counties_sf, oil_sf)\n\n### Count the number of oil spills in each county\noil_counts_sf &lt;- counties_oil_sf %&gt;% \n  group_by(name) %&gt;% \n  summarize(oil_count = n())\n\n### Plot\nggplot(oil_counts_sf) +\n  geom_sf(aes(fill = oil_count)) +\n  labs(fill = \"Number of Oil Spills\") +\n  scale_fill_gradientn(colors = c(\"white\", \"lightblue\", \"blue\", \"darkblue\")) +\n  theme_void()\n\n\n\n\n\nFigure 2: Oil spill incident counts per county in California in 2008. Darker blue values represent greater numbers of oil spills.\n\n\n\n\nThe counties with the greatest number of oil spills, in order, are Los Angeles and San Diego in Southern California, and San Mateo, Alameda, and Contra Costa in Northern California.\n\n\n\n\n\nCode\n### Convert oil spill observations to spatial point pattern (to use with spatstat package)\noil_ppp &lt;- as.ppp(oil_sf)\n\n### Set our observation window to be the extent of California\nca_counties_win &lt;- as.owin(ca_counties_sf)\n\n### Create point pattern dataset\noil_full &lt;- ppp(oil_ppp$x, oil_ppp$y, window = ca_counties_win)\n\n\n\n\nCode\n### Make a sequence of distances over which you'll calculate G(r)\nr_vec &lt;- seq(0, 20000, by = 200) \n\n### Calculate the actual and theoretical G(r) values, using 100 simulations of CSR for the \"theoretical\" outcome\ngfunction_out &lt;- envelope(oil_full, fun = Gest, r = r_vec, nsim = 100, verbose = FALSE) \n\n### Convert output to dataframe, and pivot to tidy form\ngfunction_long &lt;- gfunction_out %&gt;% \n  as.data.frame() %&gt;% \n  pivot_longer(cols = obs:hi, names_to = \"model\", values_to = \"g_val\")\n\n\n\n\nCode\n### Then make a graph in ggplot:\nggplot(data = gfunction_long, aes(x = r, y = g_val, group = model)) +\n  geom_line(aes(color = model)) +\n  scale_color_manual(values = c(\"red\", \"red\", \"blue\", \"black\"),\n                     name = \"\",\n                     labels = c(\"hi\" = \"High 95th Percentile\", \n                                \"lo\" = \"Low 95th Percentile\", \n                                \"theo\" = \"Theoretical Complete Spatial Randomness\", \n                                \"obs\" = \"Observed\")) +\n  labs(x = 'Distance (m)', y = 'G(r)') +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nFigure 3: G function for observed oil spill data (blue) in comparison with theoretical complete spatial randomness (black). Red lines indicate the 95th percentile (low and high) around the theoretical model.\n\n\n\n\n\nOur observed data is highly clustered, because the vast majority of points have a nearest neighbor that is closer than they would otherwise be in a situation with complete spatial randomness (CSR).\nThis is shown in the above graph, since the G(r) of our observations is above the G(r) of theoretical CSR."
  },
  {
    "objectID": "posts/2024-02-16-spatial-data/index.html#overview",
    "href": "posts/2024-02-16-spatial-data/index.html#overview",
    "title": "Spatial Data Analysis",
    "section": "",
    "text": "This analysis looks at inland oil spills across the state of California in 2008, as documented by the California Department of Fish and Wildlife Office of Spill Prevention and Response (OSPR).\nData source: California Department of Fish and Wildlife. Oil Spill Incident Tracking. Published Jul 29 2009. Last updated Oct 24 2023. Data download available here.\n\n\n\nThis analysis had three main goals:\n\nVisualize the locations of 2008 oil spills across the state of California\nIdentify which counties in the state had the highest number of oil spills that year\nAssess whether oil spills are spatially clustered or randomly spaced across the state\n\n\n\n\n\nImport and Clean Data\n\nRead in California counties shapefile\nRead in CSV file containing oil spill data\nConvert oil spill dataframe to simple features object\nCheck the CRS of the counties file; set oil spill sf to same CRS\n\nCreate Interactive Map\n\nCreate map of California with points denoting oil spills\nMake map interactive so the user can zoom and click on points\n\nCreate Choropleth Map\n\nSpatial join the counties with the oil spill points\nCalculate the number of oil spills in each county\nVisualize on a static choropleth map to identify counties with highest oil spill incidences\n\nPoint Pattern Analysis\n\nConvert oil spill observations into a spatial point pattern\nUse the state of California as our observation window\nCalculate actual and theoretical (complete spatial randomness) nearest neighbor distances using the G function\nPlot the G function results for our observed data and the theoretical data"
  },
  {
    "objectID": "posts/2024-02-16-spatial-data/index.html#import-and-clean-data",
    "href": "posts/2024-02-16-spatial-data/index.html#import-and-clean-data",
    "title": "Spatial Data Analysis",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(here)\nlibrary(sf)\nlibrary(tmap)\nlibrary(spatstat)\n\n\n\n\n\n\nCode\n### Read in California counties\nca_counties_sf &lt;- read_sf(here('posts', '2024-02-16-spatial-data', 'data', 'ca_counties'), layer = 'CA_Counties_TIGER2016') %&gt;%\n  janitor::clean_names() %&gt;% \n  select(name)\n\n### Read in oil spill csv\noil_df &lt;- read_csv(here('posts', '2024-02-16-spatial-data', 'data', 'oil_spill.csv')) %&gt;% \n  janitor::clean_names()\n\n\n\n\n\n\n\nCode\n### Convert lat-long oil dataframe to sf\noil_sf &lt;- oil_df %&gt;% \n  drop_na(x, y) %&gt;% \n  st_as_sf(coords = c(\"x\", \"y\"))\n\n\n\n\n\n\n\nCode\n### Check CRS of counties file\n# st_crs(ca_counties_sf) # \"EPSG\", 3857\n\n### Set oil sf CRS to CRS of CA counties\nst_crs(oil_sf) &lt;- 3857\n\n### Check CRSs are equal\n# st_crs(oil_sf) == st_crs(ca_counties_sf)"
  },
  {
    "objectID": "posts/2024-02-16-spatial-data/index.html#interactive-map",
    "href": "posts/2024-02-16-spatial-data/index.html#interactive-map",
    "title": "Spatial Data Analysis",
    "section": "",
    "text": "Code\n### set the viewing mode to interactive\ntmap_mode(mode = 'plot')\n\ntm_shape(ca_counties_sf) +\n  tm_fill(col = \"white\") +\n  tm_shape(oil_sf) +\n  tm_dots(col = \"darkblue\")\n\n\n\n\n\nFigure 1: Oil spill incidents across California in 2008. Clicking a point will reveal its incident date, location, the affected waterway, and other key information."
  },
  {
    "objectID": "posts/2024-02-16-spatial-data/index.html#choropleth-map",
    "href": "posts/2024-02-16-spatial-data/index.html#choropleth-map",
    "title": "Spatial Data Analysis",
    "section": "",
    "text": "Code\n### Spatial join counties and oil spills\ncounties_oil_sf &lt;- st_join(ca_counties_sf, oil_sf)\n\n### Count the number of oil spills in each county\noil_counts_sf &lt;- counties_oil_sf %&gt;% \n  group_by(name) %&gt;% \n  summarize(oil_count = n())\n\n### Plot\nggplot(oil_counts_sf) +\n  geom_sf(aes(fill = oil_count)) +\n  labs(fill = \"Number of Oil Spills\") +\n  scale_fill_gradientn(colors = c(\"white\", \"lightblue\", \"blue\", \"darkblue\")) +\n  theme_void()\n\n\n\n\n\nFigure 2: Oil spill incident counts per county in California in 2008. Darker blue values represent greater numbers of oil spills.\n\n\n\n\nThe counties with the greatest number of oil spills, in order, are Los Angeles and San Diego in Southern California, and San Mateo, Alameda, and Contra Costa in Northern California."
  },
  {
    "objectID": "posts/2024-02-16-spatial-data/index.html#point-pattern-analysis",
    "href": "posts/2024-02-16-spatial-data/index.html#point-pattern-analysis",
    "title": "Spatial Data Analysis",
    "section": "",
    "text": "Code\n### Convert oil spill observations to spatial point pattern (to use with spatstat package)\noil_ppp &lt;- as.ppp(oil_sf)\n\n### Set our observation window to be the extent of California\nca_counties_win &lt;- as.owin(ca_counties_sf)\n\n### Create point pattern dataset\noil_full &lt;- ppp(oil_ppp$x, oil_ppp$y, window = ca_counties_win)\n\n\n\n\nCode\n### Make a sequence of distances over which you'll calculate G(r)\nr_vec &lt;- seq(0, 20000, by = 200) \n\n### Calculate the actual and theoretical G(r) values, using 100 simulations of CSR for the \"theoretical\" outcome\ngfunction_out &lt;- envelope(oil_full, fun = Gest, r = r_vec, nsim = 100, verbose = FALSE) \n\n### Convert output to dataframe, and pivot to tidy form\ngfunction_long &lt;- gfunction_out %&gt;% \n  as.data.frame() %&gt;% \n  pivot_longer(cols = obs:hi, names_to = \"model\", values_to = \"g_val\")\n\n\n\n\nCode\n### Then make a graph in ggplot:\nggplot(data = gfunction_long, aes(x = r, y = g_val, group = model)) +\n  geom_line(aes(color = model)) +\n  scale_color_manual(values = c(\"red\", \"red\", \"blue\", \"black\"),\n                     name = \"\",\n                     labels = c(\"hi\" = \"High 95th Percentile\", \n                                \"lo\" = \"Low 95th Percentile\", \n                                \"theo\" = \"Theoretical Complete Spatial Randomness\", \n                                \"obs\" = \"Observed\")) +\n  labs(x = 'Distance (m)', y = 'G(r)') +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nFigure 3: G function for observed oil spill data (blue) in comparison with theoretical complete spatial randomness (black). Red lines indicate the 95th percentile (low and high) around the theoretical model.\n\n\n\n\n\nOur observed data is highly clustered, because the vast majority of points have a nearest neighbor that is closer than they would otherwise be in a situation with complete spatial randomness (CSR).\nThis is shown in the above graph, since the G(r) of our observations is above the G(r) of theoretical CSR."
  },
  {
    "objectID": "posts/2024-02-02-time-series/index.html",
    "href": "posts/2024-02-02-time-series/index.html",
    "title": "Time Series Analysis",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(here)\nlibrary(lubridate)\nlibrary(tsibble)\nlibrary(feasts)\nlibrary(fable)\n\n\n\nOverview\n\n\n\n\n\nWillamette Falls in Oregon City, OR. Photo credit: PGE\n\n\n\n\n\n\n\nChute to facilitate fish passage over the falls. Photo credit: PGE\n\n\n\n \n\n\n\n\nCoho salmon swimming upstream. Photo credit: Wild Salmon Center\n\n\n\n\nThis report investigates the passage of coho salmon, coho jacks, and steelhead trout through Willamette Falls from January 2001 through December 2010. The Willamette Falls Fishway consists of three fish ladders that facilitate the movement of fish across the waterfalls. This passage is especially important for migratory species, such as salmon and steelhead trout, which must migrate upstream to spawn. The data used for these analyses come from scientific observations of the fish passing through the ladders of this fishway.\nData source: Columbia River DART. DART Adult Passage Counts (Willamette Falls, 2001 - 2010). https://www.cbr.washington.edu/dart/query/adult_graph_text. Accessed January 25, 2023.\n\n\nCode\n### Read in the data\nfish_df &lt;- read_csv(here('posts/2024-02-02-time-series/data/willamette_fish_passage.csv')) %&gt;% \n  janitor::clean_names()\n\n\n\n\nCode\n### Format data as time series\nfish_ts &lt;- fish_df %&gt;% \n  mutate(date = lubridate::mdy(date)) %&gt;% \n  as_tsibble(key = NULL,\n             index = date)\n\n\n\nTime SeriesSeasonplotAnnual Counts\n\n\n\n\nCode\n### Tidy time series data, with selected species\nfish_ts_select &lt;- fish_ts %&gt;% \n  select(date, coho, jack_coho, steelhead) %&gt;% \n  pivot_longer(cols = coho:steelhead,\n               names_to = \"species\",\n               values_to = \"count\") %&gt;% \n  mutate(count = replace_na(count, 0))\n\n\n\n\nCode\nggplot(data = fish_ts_select, aes(x = date, y = count, color = species)) +\n  geom_line() +\n  labs(x = \"Date\", y = \"Number of Fish\", title = \"Adult Fish Passage at Willamette Falls\") +\n  scale_color_manual(values = c('darkslategray4', 'darkslategray3', 'darkseagreen3')) +\n  facet_wrap(~ species,\n             nrow = 3,\n             labeller = labeller(species = \n                                   c(\"coho\" = \"Coho\",\n                                     \"jack_coho\" = \"Jack Coho\",\n                                     \"steelhead\" = \"Steelhead\")),\n             scales = \"free_y\") +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  scale_y_continuous(labels = scales::comma_format()) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\nFigure 1: Counts of fish passing through Willamette Falls fishway by type (coho, jack coho, and steelhead), from 2001 to 2010.\n\n\n\n\n\nAll examined fish species pass through the falls annually, at fairly regular intervals.\nThe peak passage times for coho and jack coho are aligned, and happen within a fairly short time window.\nSteelhead pass through the falls across a wider time window, with less well-defined peaks, compared to coho and jack coho counts.\n\n\n\n\n\nCode\nfish_ts_select %&gt;% \ngg_season(y = count, pal = c(\"darkslategray\",\"darkslategray2\")) +\n  labs(x = \"Date\", y = \"Number of Fish\", \n       title = \"Seasonplot of Adult Fish Passage at Willamette Falls\") +\n  facet_wrap(~ species,\n             nrow = 3,\n             labeller = labeller(species = \n                                   c(\"coho\" = \"Coho\",\n                                     \"jack_coho\" = \"Jack Coho\",\n                                     \"steelhead\" = \"Steelhead\")),\n             scales = \"free_y\") +\n  scale_x_date(date_breaks = \"1 month\", date_labels = \"%b\") +\n  scale_y_continuous(labels = scales::comma_format()) +\n  theme_minimal()\n\n\n\n\n\nFigure 2: Fish counts at Willamette Falls by season and fish type, from 2001 to 2010. Counts in later years are signified by lighter colors of blue.\n\n\n\n\n\nThe peak passage times for coho and jack coho are very well aligned in the fall of every year.\nIn the most recent years, observed peak counts for coho and jack coho have been higher than in the earlier 2000s.\nSteelhead pass through the falls during many months of the year, with peak fish counts generally in the late spring and early summer months. Peak dates for steelhead passage seem to have shifted slightly later in recent years compared to in earlier years.\n\n\n\n\n\nCode\n### Group data by fish type and year\nfish_ts_annual &lt;- fish_ts_select %&gt;% \n  index_by(year = ~year(.)) %&gt;% \n  group_by(species) %&gt;% \n  summarize(annual_count = sum(count))\n\n\n\n\nCode\nggplot(data = fish_ts_annual, aes(x = as_factor(year), y = annual_count, color = species, group = species)) +\n  geom_point() +\n  geom_line() +\n  labs(x = \"Date\", y = \"Number of Fish\", title = \"Annual Counts of Adult Fish Passage at Willamette Falls\") +\n  scale_color_manual(values = c('darkslategray4', 'darkslategray3', 'darkseagreen3')) +\n  facet_wrap(~ species,\n             nrow = 3,\n             labeller = labeller(species = \n                                   c(\"coho\" = \"Coho\",\n                                     \"jack_coho\" = \"Jack Coho\",\n                                     \"steelhead\" = \"Steelhead\")),\n             scales = \"free_y\") +\n  scale_y_continuous(labels = scales::comma_format()) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\nFigure 3: Cumulative counts of each fish type (coho, jack coho, and steelhead) across each year of the study period.\n\n\n\n\n\nCoho numbers increased noticeably in 2009 and 2010 compared to earlier years.\nJack coho numbers have fluctuated over the study period. It is unclear whether and how they are correlated to coho counts.\nSteelhead numbers generally declined through the 2000s. However, in 2010 the counts appeared to increase, potentially signaling a rebound in population growth."
  },
  {
    "objectID": "posts/2024-01-30-data-wrangling/index.html",
    "href": "posts/2024-01-30-data-wrangling/index.html",
    "title": "Data Wrangling and Visualization",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(here)\nlibrary(readxl)\nlibrary(lubridate)\nlibrary(cowplot)"
  },
  {
    "objectID": "posts/2024-01-30-data-wrangling/index.html#read-in-data",
    "href": "posts/2024-01-30-data-wrangling/index.html#read-in-data",
    "title": "Data Wrangling and Visualization",
    "section": "1.1 Read in data",
    "text": "1.1 Read in data\nData was obtained from:\nKnapp, R.A., C. Pavelka, E.E. Hegeman, and T.C. Smith. 2020. The Sierra Lakes Inventory Project: Non-Native fish and community composition of lakes and ponds in the Sierra Nevada, California ver 2. Environmental Data Initiative. https://doi.org/10.6073/pasta/d835832d7fd00d9e4466e44eea87fab3\n\n\nCode\namphib_df &lt;- read_excel(here('posts', '2024-01-30-data-wrangling', 'data', 'sierra_amphibians.xlsx')) %&gt;% \n  janitor::clean_names()"
  },
  {
    "objectID": "posts/2024-01-30-data-wrangling/index.html#data-wrangling",
    "href": "posts/2024-01-30-data-wrangling/index.html#data-wrangling",
    "title": "Data Wrangling and Visualization",
    "section": "2.1 Data wrangling",
    "text": "2.1 Data wrangling\n\n\nCode\nramu_df &lt;- amphib_df %&gt;% \n  mutate(date = lubridate::ymd(survey_date)) %&gt;% # Convert survey date column to date format\n  mutate(year = lubridate::year(date)) %&gt;% # Extract year from the date column\n  filter(amphibian_species == 'RAMU' & \n         amphibian_life_stage != 'EggMass') %&gt;% # Filter for just Rana muscosa and exclude egg-mass observations\n  group_by(year, amphibian_life_stage) %&gt;% \n  summarize(ramu_count = sum(amphibian_number)) %&gt;% # Add together the number of observed frogs in each year and life stage\n  ungroup()"
  },
  {
    "objectID": "posts/2024-01-30-data-wrangling/index.html#visualization",
    "href": "posts/2024-01-30-data-wrangling/index.html#visualization",
    "title": "Data Wrangling and Visualization",
    "section": "2.2 Visualization",
    "text": "2.2 Visualization\nI built a plot that shows Rana muscosa counts by life stage across each year of the study period.\n\n\nCode\nramu_plot &lt;- ggplot(data = ramu_df, aes(x = year, y = ramu_count, fill = amphibian_life_stage)) +\n  geom_col(position = 'stack') +\n  labs(x = 'Year', y = 'Count', fill = 'Life Stage') +\n  ggtitle(expression(paste('Annual counts of ', italic('Rana muscosa'), ' by life stage'))) +\n  scale_fill_manual(values = c('darkslategray4', 'darkslategray2', 'darkseagreen3')) +\n  scale_x_continuous(n.breaks = 8) +\n  scale_y_continuous(labels = scales::comma_format()) +\n  theme_classic() +\n  theme(legend.position = c(0.11, 0.85))"
  },
  {
    "objectID": "posts/2024-01-30-data-wrangling/index.html#data-wrangling-1",
    "href": "posts/2024-01-30-data-wrangling/index.html#data-wrangling-1",
    "title": "Data Wrangling and Visualization",
    "section": "3.1 Data wrangling",
    "text": "3.1 Data wrangling\n\n\nCode\nramu_lakes_df &lt;- amphib_df %&gt;% \n  filter(amphibian_species == 'RAMU' & \n         amphibian_life_stage %in% c('Adult', 'SubAdult')) %&gt;% # Filter for just Rana muscosa and just adult/subadults\n  group_by(lake_id) %&gt;% \n  summarize(count = sum(amphibian_number)) %&gt;% # Add together the number of observed frogs\n  ungroup() %&gt;% \n  slice_max(count, n = 5) %&gt;% # Take the 5 lakes with the largest counts\n  mutate(lake_id = as.factor(lake_id))"
  },
  {
    "objectID": "posts/2024-01-30-data-wrangling/index.html#visualization-1",
    "href": "posts/2024-01-30-data-wrangling/index.html#visualization-1",
    "title": "Data Wrangling and Visualization",
    "section": "3.2 Visualization",
    "text": "3.2 Visualization\nI built a second plot, this time to show the counts of Rana muscosa in each of the five lakes where it was most numerous. Lake IDs are used instead of specific lake names, in order to keep these locations confidential.\n\n\nCode\nramu_lakes_plot &lt;- ggplot(data = ramu_lakes_df, aes(y = fct_reorder(lake_id, count, .desc = FALSE), x = count)) +\n  geom_col(fill = 'darkslategray3') +\n  labs(x = 'Count (Adults & Subadults)', y = 'Lake ID') +\n  ggtitle(expression(paste('Five lakes with highest counts of ', italic('Rana muscosa')))) +\n  scale_x_continuous(labels = scales::comma_format()) +\n  theme_classic()"
  },
  {
    "objectID": "posts/2024-03-17-clustering/index.html",
    "href": "posts/2024-03-17-clustering/index.html",
    "title": "Hierarchical Clustering",
    "section": "",
    "text": "The data used for this analysis were collected for the Santa Barbara Coastal Long Term Ecological Research Network (SBC LTER). This dataset consists of water chemistry measurements taken from streams in Santa Barbara beginning in 2000. Key variables measured include ammonium, nitrate, phosphorous, nitrogen and phosphorous (dissolved), carbon, nitrogen, & phosphorous (particulate), total suspended solids, and conductivity.\nData source:\nSanta Barbara Coastal LTER and J. Melack. 2019. SBC LTER: Land: Stream chemistry in the Santa Barbara Coastal drainage area, ongoing since 2000 ver 16. Environmental Data Initiative. https://doi.org/10.6073/pasta/67a558a24ceed9a0a5bf5e46ab841174\n\n\n\nThe purpose of this analysis is to cluster stream sites based upon their water chemistry similarities and dissimilarities. There are 13 study sites in the data set, located along various streams. Clustering may reveal patterns in the data and similarities between sites that may be otherwise hard to observe given the many water quality parameters measured.\n\n\n\nI will use agglomerative (bottom-up) hierarchical clustering to group together individual stream sites based upon similarity. Specifically, I use the following analytical process:\n\nGet and tidy data\nRemove water chemistry variables with many NAs (&gt; 50% of observations)\nCalculate, for each stream site, the mean values for the remaining water chemistry parameters. Remove remaining NAs in the process\nScale the data\nCreate Euclidean distance matrix\nPerform hierarchical clustering by complete linkage\nPerform hierarchical clustering by single linkage and compare with the complete linkage results"
  },
  {
    "objectID": "posts/2024-03-17-clustering/index.html#data-summary",
    "href": "posts/2024-03-17-clustering/index.html#data-summary",
    "title": "Hierarchical Clustering",
    "section": "",
    "text": "The data used for this analysis were collected for the Santa Barbara Coastal Long Term Ecological Research Network (SBC LTER). This dataset consists of water chemistry measurements taken from streams in Santa Barbara beginning in 2000. Key variables measured include ammonium, nitrate, phosphorous, nitrogen and phosphorous (dissolved), carbon, nitrogen, & phosphorous (particulate), total suspended solids, and conductivity.\nData source:\nSanta Barbara Coastal LTER and J. Melack. 2019. SBC LTER: Land: Stream chemistry in the Santa Barbara Coastal drainage area, ongoing since 2000 ver 16. Environmental Data Initiative. https://doi.org/10.6073/pasta/67a558a24ceed9a0a5bf5e46ab841174"
  },
  {
    "objectID": "posts/2024-03-17-clustering/index.html#purpose",
    "href": "posts/2024-03-17-clustering/index.html#purpose",
    "title": "Hierarchical Clustering",
    "section": "",
    "text": "The purpose of this analysis is to cluster stream sites based upon their water chemistry similarities and dissimilarities. There are 13 study sites in the data set, located along various streams. Clustering may reveal patterns in the data and similarities between sites that may be otherwise hard to observe given the many water quality parameters measured."
  },
  {
    "objectID": "posts/2024-03-17-clustering/index.html#analytical-outline",
    "href": "posts/2024-03-17-clustering/index.html#analytical-outline",
    "title": "Hierarchical Clustering",
    "section": "",
    "text": "I will use agglomerative (bottom-up) hierarchical clustering to group together individual stream sites based upon similarity. Specifically, I use the following analytical process:\n\nGet and tidy data\nRemove water chemistry variables with many NAs (&gt; 50% of observations)\nCalculate, for each stream site, the mean values for the remaining water chemistry parameters. Remove remaining NAs in the process\nScale the data\nCreate Euclidean distance matrix\nPerform hierarchical clustering by complete linkage\nPerform hierarchical clustering by single linkage and compare with the complete linkage results"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Olivia Hemond",
    "section": "",
    "text": "Hello!\nI am an environmental scientist passionate about finding equitable and actionable solutions for the climate and biodiversity crises.\n\n\nProfessional Interests\nConservation planning | Climate adaptation & resilience | Urban and regional planning | Nature-based solutions\n\n\nSkillsets\nData science | Data visualization | Spatial analysis | GIS | Communications | Human-centered design\n\n\nCurrent Projects\nClimate Adaptation for Forest Management (link coming soon)\nMapping Urban Heat Risk Inequality in LA (link coming soon)"
  }
]